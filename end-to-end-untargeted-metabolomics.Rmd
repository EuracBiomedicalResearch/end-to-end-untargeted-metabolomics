---
title: "End-to-end workflow for LC-MS/MS analysis using *RforMassSpectrometry* and *xcms*"
author:
  - name: "Philippine Louail"
    affiliation: "Institute for Biomedicine, Eurac Research, Bolzano, Italy"
  - name: "Anna Tagliaferri"
    affiliation: "Institute for Biomedicine, Eurac Research, Bolzano, Italy"
  - name: "Vinicius Verri Hernandes"
    affiliation: "Institute for Biomedicine, Eurac Research, Bolzano, Italy"
  - name: "Johannes Rainer"
    affiliation: "Institute for Biomedicine, Eurac Research, Bolzano, Italy"
output: html_document
date: "2023-09-07"
bibliography: references.bib
---

```{r style, warning=FALSE, include=FALSE, results='hide'}
library("BiocStyle")
library("knitr")
library("rmarkdown")
opts_chunk$set(message = FALSE, error = FALSE, warning = FALSE,
               cache = FALSE)
```

# Abstract

Metabolomics provides a real-time view of the metabolic state of examined
samples, with mass spectrometry serving as a key tool in deciphering intricate
differences in metabolomes due to specific factors. In the context of
metabolomic investigations, untargeted LC-MS/MS emerges as a powerful approach.
This paper focuses on a dataset aimed at identifying differences in plasma
metabolite levels between individuals suffering from a cardiovascular disease
and healthy controls.

Despite the potential insights offered by untargeted LC-MS/MS data, a
significant challenge in the field lies in the generation of reproducible and
scalable analysis workflows. Various software packages, also with convenient
graphical user interfaces for the analysis of such data exist, but high variety
of different chromatographic setups as well as MS acquisition modes and
configurations used in untargeted metabolomics demands highly flexible and
customizable software packages and analysis workflows [jo: the previous sentence
needs rewriting - or maybe we remove it completely]. The power of R-based
analysis workflows lies in their high customizability and adaptability to
specific instrumental and experimental setups, but, while various specialized
packages exist for individual analysis steps, their seamless integration and
application to large cohort datasets remains elusive.  Addressing this gap, we
present an innovative R workflow that leverages *xcms*, and packages of the
*RforMassSpectrometry* environment to encompass all aspects of pre-processing
and downstream analyses for LC-MS/MS datasets in a reproducible manner and allow
an easy customization to generate data-set specific workflows. Our pipeline
seamlessly integrates with Bioconductor packages, offering adaptability to
diverse study designs and analysis requirements.

# Keyword
LC-MS/MS, reproducibility, workflow, xcms, R, normalization,
feature identification, Bioconductor,...

# Introduction

LC-MS/MS is a powerful tool for metabolomics investigations, providing a
comprehensive view of the metabolome. It enables the identification of
a large number of metabolites and their relative abundance in biological samples. The high
sensitivity and specificity of LC-MS/MS make it an indispensable tool for
biomarker discovery and elucidating metabolic pathways. The untargeted approach
is particularly useful for hypothesis-free investigations, allowing for the
detection of unexpected metabolites and pathways. However, the analysis of
LC-MS/MS data is complex and requires a series of preprocessing steps to
extract meaningful information from the raw data. The main challenges are to
deal with the lack of ground truth data, the high dimensionality of the data,
and the presence of noise and artifacts. Moreover, due to different instrumental
setups and used protocols the definition of a single *one-fits-all* workflow is
impossible. Finally, while specialized software packages exist for each
individual step during an analysis, their seamless integration remains elusive.

The *RforMassSpectrometry* initiative aims to implement an expandable, flexible
infrastructure for the analysis of MS data, providing also a comprehensive
toolbox of functions to build customized analysis workflows.

Here we present a complete analysis workflow for untargeted LC-MS/MS data using
R and Bioconductor packages, in particular those from the *RforMassSpectrometry*
package ecosystem. We demonstrate how the various algorithms can be adapted to
the particular data set and how various R packages can be seamlessly integrated
to ensure efficient and reproducible processing. The present workflow covers all
steps for LC-MS/MS data analysis, from preprocessing, data normalization,
differential abundance analysis and annotation of the significant
*features*. Various options for visualizations as well as quality assessment are
presented for all analysis steps.

[jo: maybe we can have a table of all used R/Bioconductor packages and for what
they were used]


# Data description

The datasets used in this workflow are an LC-MS-based untargeted metabolomics
data set to quantify small polar metabolites in human plasma samples and an
additional LC-MS/MS data set of selected samples from the former study for the
identification/annotation of its significant features. The samples used here
were randomly selected from a larger study for the identification of metabolites
with differences in abundances between individuals suffering from a
cardiovascular disease (CVD) and healthy controls (CTRL). The subset analyzed
here comprises data for 3 CVD and 3 CTRL as well as four quality control (QC)
samples. The QC samples represent a pool of serum samples from a large cohort
and were repeatedly measured throughout the experiment to monitor stability of
the signal.

The data and metadata for this workflow are accessible on the massive database
under the ID: [ID].


# Workflow description

The present workflow describes all steps for the analysis of an LC-MS/MS
experiment, which includes the preprocessing of the raw data to generate the
*abundance* matrix for the *features* in the various samples, followed by data
normalization, differential abundance analysis and finally the annotation of
features to metabolites. The packages are listed in table XXX [jo: maybe provide
that info as a table]. Note that also alternative analysis options and R
packages could be used for different steps and some examples are mentioned
throughout the workflow.  [jo: I'll include some of these maybe later. It would
be key to justify why this workflow is comprehensive]

Our workflow is therefore based on the following dependencies:

```{r packages used, message=FALSE}
library(MsExperiment)
library(xcms)
library(Spectra)
library(RColorBrewer)
library(pander)
library(readxl)
library(MetaboCoreUtils)
library(pheatmap)
library(Biobase)
library(MetaboAnnotation)
library(ggfortify)
library(matrixStats) # summaries over matrices
library(SummarizedExperiment)
```

# Data import

The *mzML* files with the raw MS data are located within the *data/mzML* folder
of this repository. [ideally, they should be added and then downloaded from
MetaboLight].

Data import from metaboloight first need to be extracted using the *isatabr*
package. (hjere there I need to write an import command specific fro ISA-tab file.)

```{r import}
#' read the sample descriptions from an xlsx sheet
pd <- read_xlsx("data/phenodata-lc-ms.xlsx") |>
    as.data.frame()

#' Import the data
#' Massive data bank extraction but for now not.
#' MZML_PATH <- "data/metabolights/mzML/"
MZML_PATH <- "C:/Users/plouail/OneDrive - Scientific Network South Tyrol/end-to-end_worflow/data/mzML_files/MS1"

data <- readMsExperiment(file.path(MZML_PATH, "/", pd$mzML_file),
                         sampleData = pd)
```

We next configure the parallel processing setup. Most functions from the *xcms*
package allow per-sample parallel processing, which can improve the performance
of the analysis, especially for large data sets. *xcms* and all packages from
the *RforMassSpectrometry* package ecosystem use the parallel processing setup
configured through the `r Biocpkg("BiocParallel")` Bioconductor package. With
the code below we use a *fork-based* parallel processing on unix system, and a
*socket-based* parallel processing on the Windows operating system.

```{r par-process}
#' Set up parallel processing using 2 cores
if (.Platform$OS.type == "unix") {
    register(MulticoreParam(2))
} else
    register(SnowParam(2))
}
```

# Data organisation

The experimental data is now represented by a `MsExperiment` object from the `r
Biocpkg("MsExperiment")` package. The `MsExperiment` object is a container for
metadata and spectral data that provides and manages also the linkage between
samples and spectra.

```{r}
data
```

Below we provide a brief overview of the data structure and content. The
`sampleData()` function extracts sample information from the object. We next
extract this data and use the *pander* package to render and show this
information in Table 1 below. Throughout the document we use the R pipe operator
(`|>`) to avoid nested function calls and hence improve code readability.

```{r phenodata, echo=FALSE}
#' Extract selected columns from the sample data and create a table
sampleData(data)[, c("mzML_file", "injection_index", "phenotype",
                     "sample_name", "age")] |>
    as.data.frame() |>
    pandoc.table(style = "rmarkdown",
                 caption = "Table 1. Samples from the data set.")
```

There are `r length(sampleData(data))` samples in this data set. Below are
abbreviations essential for proper interpretation of this sample information:

- *injection_index*: An index representing the order (position) in which an
  individual sample was measured (injected) within the LC-MS measurement run of
  the experiment.
- *phenotype*: The sample groups of the experiment:
  - `"QC"`: Quality control sample (pool of serum samples from an external,
    large cohort).
  - `"CVD"`: Sample from an individual with a cardiovascular disease.
  - `"CTR"`: Sample from presumably healthy control.
- *sample_name*: An arbitrary name/identifier of the sample.
- *age*: The (rounded) age of the individuals.

We will define colors for each of the sample groups based on their sample
group using the *RColorBrewer* package:

```{r define-colors, include=FALSE}
#' Define colors for the different phenotypes
col_phenotype <- brewer.pal(9, name = "Set1")[c(9, 5, 4)]
names(col_phenotype) <- c("QC", # grey
                          "CVD", # orange
                          "CTR") # purple
col_sample <- col_phenotype[sampleData(data)$phenotype]
```

The MS data of this experiment is stored as a `Spectra` object (from the
`r Biocpkg("Spectra")` Bioconductor package) within the `MsExperiment` object
and can be accessed using `spectra()` function. Each element in this object is a
spectrum - they are organised linearly and are all combined in the same
`Spectra` object one after the other (ordered by retention time and samples).
Below we access the data set's `Spectra` object which will summarize its
available information available and provide, among other, the total number of
spectra of the data set.

```{r}
#' Access Spectra Object
spectra(data)
```

We can also summarize the number of spectra and their respective MS level
(extracted with the `msLevel()` function. The `fromFile()` function returns for
each spectrum the index of its sample (data file) and can thus be used to split
the information (MS level in this case) by sample to further summarize using the
base R `table()` function and combine the result into a matrix.

```{r}
#' Count the number of spectra with a specific MS level per file.
spectra(data) |>
    msLevel() |>
    split(fromFile(data)) |>
    lapply(table) |>
    do.call(what = cbind)
```

The present data set thus contains only MS1 data, which is ideal for
quantification of the signal. A second (LC-MS/MS) data set also with fragment
(MS2) spectra of the same samples will be used later on in the workflow.

Note that users should not restrict themselves to data evaluation examples shown
here or in other tutorials. The *Spectra* package enables an user-friendly
access to the full MS data and its functionality should be extensively used to
explore, visualize and summarize the data.

As another example, we below determine the retention time range for the entire
data set.

```{r}
#' Retention time range for entire dataset
spectra(data) |>
    rtime() |>
    range()
```

Data obtained from LC-MS experiments are typically analyzed along the retention
time axis, while MS data is organized by spectrum, orthogonal to the retention
time axis. The `chromatogram()` function facilitates the extraction of
intensities along the retention time. However, access to chromatographic
information is currently not as efficient and seamless as it is for spectral
information. Work is underway to develop/improve the infrastructure for
chromatographic data through a new `Chromatograms` object aimed to be as
flexible and user-friendly as the `Spectra` object.


# Data visualization and general quality assessment

Effective visualization is paramount for inspecting and assessing the quality of
MS data. For a general overview of our LC-MS data, we can:

- Combine all mass peaks from all (MS1) spectra of a sample into a single
  spectrum in which each mass peak then represents the maximum signal of all
  mass peaks with a similar mass-to-charge ratio (*m/z*). This spectrum might
  then be called Base Peak Spectrum (BPS), providing information on the most
  abundant ions of a sample.
- Aggregate mass peak intensities for each spectrum, resulting in the Base Peak
  Chromatogram (BPC). The BPC shows the highest measured intensity for each
  distinct retention time (hence spectrum) and is thus orthogonal to the BPS,
  which reports for each *m/z* the highest intensity along the retention time
  range.
- Sum mass peak intensities for each spectrum to create a Total Ion Chromatogram
  (TIC).
- Compare the BPS of all samples in an experiment to evaluate similarity of
  their ion content.
- Compare the BPC of all samples in an experiment to identify samples with
  similar or dissimilar chromatographic signal.

In addition to such general data evaluation and visualization, it is also
crucial to investigate specific signal of e.g. internal standards or
compounds/ions known to be present in the samples.


## Spectra Data Visualization

The BPS collapses data in the retention time dimension, providing insights into
the most abundant mass-to-charge values (*m/z*) in the dataset, irrespective of
the retention time in which they were measured. Creation of such BPS is however
not straightforward. Mass peaks, even if representing signals from the same ion,
will never have identical *m/z* values in consecutive spectra due to the
measurement error/resolution of the instrument. Below we use the
`combineSpectra` function to combine all spectra from one file (defined using
parameter `f = fromFile(data)`) into a single spectrum. All mass peaks with a
difference in *m/z* value smaller than 3 parts-per-million (ppm) are combined
into one mass peak, with an intensity representing the maximum of all such
grouped mass peaks. To reduce memory requirement we in addition first *bin* each
spectrum combining all mass peaks within a spectrum aggregating mass peaks into
bins with 0.01 *m/z* width. In case of large datasets, it is also recommended to
set the `processingChunkSize()` parameter of the `MsExperiment` object to a
finite value (default is `Inf`) which will cause the data to be processed (and
loaded into memory) in chunks of `processingChunkSize()` spectra. This can
reduce memory demand and speed up the process.

```{r}
#' Setting the chunksize
chunksize <- 10000
processingChunkSize(spectra(data)) <- chunksize
```

We can now generate BPS for each sample and `plot()` them.

```{r bps, echo=TRUE, fig.width = 12, fig.height = 7, fig.cap = "Base peak spectra for the 10 samples of the experiment."}
#' Combining all spectra per file into a single spectrum
bps <- spectra(data) |>
    bin(binSize = 0.01) |>
    combineSpectra(f = fromFile(data), intensityFun = max, ppm = 3)

#' Plot the base peak spectra
par(mar = c(2, 1, 1, 1))
plotSpectra(bps)
```

The Base Peak Spectra (BPS) reveal the most prevalent ions present in each of
the samples. Here, there is observable overlap in ion content between the files,
particularly around 300 *m/z* and 700 *m/z*. There are however also differences
between sets of samples. In particular, BPS 1, 4, 7 and 10 (counting row-wise
from left to right) seem different than the others. In fact, these 4 BPS are
from QC samples, and the remaining 6 from the study samples. The observed
differences might be explained by the fact that the QC samples are pools of
serum samples from a different cohort, while the study samples represent plasma
samples, from a different sample collection.

Next to the visual inspection above, we can also calculate and express the
similarity between the BPS with a heatmap. Below we use the `compareSpectra()`
function to calculate pairwise similarities between all BPS and use then the
`pheatmap()` function from the *pheatmap* package to cluster and visualize this
result.

```{r compare-spectra, echo=TRUE}
#' Calculate similarities between BPS
sim_matrix <- compareSpectra(bps)

#' Add rownames and colnames
rownames(sim_matrix) <- colnames(sim_matrix) <- sampleData(data)$sample_name
ann <- data.frame(phenotype = sampleData(data)[, "phenotype"])
rownames(ann) <- rownames(sim_matrix)

#' plot the heatmap
pheatmap(sim_matrix, annotation_col = ann,
         annotation_colors = list(phenotype = col_phenotype))
```

We get a first glance at how our different samples distribute in terms of
similarity. The heatmap above shows that the QC samples are quite different from
the study samples. This is expected as the QC samples are a pool of serum
samples from a larger cohort, while the study samples are plasma samples from
participants from a different study (and sample collection).

It is also strongly recommended to delve deeper into the data by exploring it in
more detail. This can be accomplished by carefully assessing our data and
extracting spectra or regions of interest for further examination. Here we will
now look at how to extract information for specific spectrum from specific
samples.

```{r }
#' Accessing a single spectrum - comparing with QC
par(mfrow = c(1,2), mar = c(2, 2, 2, 2))
spec1 <- spectra(data[1])[125]
spec2 <- spectra(data[3])[125]
plotSpectra(spec1, main = "125th spectrum of one QC sample")
plotSpectra(spec2, main = "125th spectrum of one CTR sample")
```

There is indeed a significant difference in peak distribution and intensity
between the QC sample and the CTR sample. This further illustrates the apparent
difference in signal between the QC and study samples from this data set. We
next compare also

```{r}
#' Accessing a single spectrum - comparing CVD and CTR
par(mfrow = c(1,2), mar = c(2, 2, 2, 2))
spec1 <- spectra(data[2])[125]
spec2 <- spectra(data[3])[125]
plotSpectra(spec1, main = "125th spectrum of one CVD sample")
plotSpectra(spec2, main = "125th spectrum of one CTR sample")
```

Above, we can observe that the spectra between CVD and CTR samples are not
entirely similar, but they do exhibit similar main peaks between 200 and 600
m/z with a general higher intensity in control samples.
However the peak distribution (or at least intensity) seems to vary the most
between an *m/z* of 10 to 210 and after an *m/z* of 600.

The CTR spectrum above exhibits significant peaks around an *m/z* of 150 - 200
that have a much lower intensity in the CVD sample. To delve into more details
about this specific spectrum, a wide range of functions can be employed:

```{r}
#' Checking its intensity
intensity(spec2)

#' Checking its rtime
rtime(spec2)

#' Checking its m/z
mz(spec2)
```

[jo: removed this whole part. I don't think it did contribute to the
comprehension of the data and might be just confusing]


## Chromatographic Data Visualization

For visualizing LC-MS data, a Base Peak Chromatogram (BPC) or Total Ion
Chromatogram (TIC) serves as a valuable tool to assess the performance of liquid
chromatography across various samples in an experiment. In our case, we extract
the BPC from our data to create such a plot. The BPC captures the maximum peak
signal from each spectrum in a data file and plots this information against the
retention time for that spectrum on the y-axis. The BPC can be extracted using
the `chromatogram` function.

By setting the parameter `aggregationFun = "max"`, we instruct the function to
report the maximum signal per spectrum. Conversely, when setting
`aggregationFun = "sum"`, it sums up all intensities of a spectrum, thereby
creating a Total Ion Chromatogram (TIC).

```{r bpc1, echo=TRUE}
#' Extract and plot BPC for full data
bpc <- chromatogram(data, aggregationFun = "max")

plot(bpc, col = paste0(col_sample, 80), main = "BPC", lwd = 1.5)
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, lwd = 2, horiz = TRUE, bty = "n")
```

After about 240 seconds no signal seems to be measured. Thus, we below filter
the data removing that part as well as the first 10 seconds measured in the LC
run.

```{r filter rt}
#' Filter the data based on retention time
data <- filterRt(data, c(10, 240))
bpc <- chromatogram(data, aggregationFun = "max")

#' Plot after filtering
plot(bpc, col = paste0(col_sample, 80),
     main = "BPC after filtering retention time", lwd = 1.5)
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, lwd = 2, horiz = TRUE, bty = "n")
```

Initially, we examined the entire Base Peak Chromatogram (BPC) and subsequently
filtered it based on specific retention times. This not only results in a
smaller file size but also facilitates a more straightforward interpretation of
the BPC.

The final plot illustrates the BPC for each sample colored by phenotype,
providing insights on the signal measured along the retention times of each sample. It
reveals the points at which compounds eluted from the LC column. In essence, a
BPC condenses the 3-dimensional LC-MS data (*m/z* by retention time by intensity)
into 2 dimensions (retention time by intensity).

We can also here compare similarities of the BPCs in a heatmap. The retention
times will however not be identical between different samples. Thus we *bin* the
chromatographic signal per sample along the retention time axis into bins of 2
seconds resulting in data with the same number of bins/data points. We can then
calculate pairwise similarities between these data vectors using the `cor()`
function and visualize the result using a heatmap.

```{r heatmap1, eval=FALSE}
#' Total ion chromatogram
tic <- chromatogram(data, aggregationFun = "sum") |>
  bin(binSize = 2)

#' Calculate similarity (Pearson correlation) between BPCs
ticmap <- do.call(cbind, lapply(tic, intensity)) |>
  cor()

rownames(ticmap) <- colnames(ticmap) <- sampleData(data)$sample_name
ann <- data.frame(phenotype = sampleData(data)[, "phenotype"])
rownames(ann) <- rownames(ticmap)

#' Plot heatmap
pheatmap(ticmap, annotation_col = ann,
         annotation_colors = list(phenotype = col_phenotype))
```

The heatmap above reinforces what our exploration of spectra data showed, which
is a strong separation between the QC and study samples. This is important to
bear in mind for later analysis.

Additionally, study samples group into two clusters, cluster *I* containing
samples *C* and *F* and cluster *II* with all other samples. Below we plot the
TIC of all samples, using a different color for each cluster.

```{r}
cluster_I_idx <- sampleData(data)$sample_name %in% c("F", "C")
cluster_II_idx <- sampleData(data)$sample_name %in% c("A", "B", "D", "E")

temp_col <- c("grey", "red")
names(temp_col) <- c("Cluster II", "Cluster I")
col <- temp_col[outlier_idx + 1]
col[sampleData(data)$phenotype == "QC"] <- NA

data |>
    chromatogram(aggregationFun = "sum") |>
    plot( col = col,
     main = "BPC after filtering retention time", lwd = 1.5)
grid()
legend("topright", col = temp_col,
       legend = names(temp_col), lty = 1, lwd = 2,
       horiz = TRUE, bty = "n")
```

While over and above the TIC of all samples look similar, the samples from
cluster I show a different signal in the retention time range from about 40 to
160 seconds. Whether, and how strong this difference will impact the following
analysis remains however to be determined.


### Known compounds

Throughout the entire process, it is crucial to have reference points within the
dataset, such as well-known ions. Most experiments nowadays include internal
standards (IS), and it was the case here. We strongly recommend using them for
visualization throughout the entire analysis. For this experiment, a set of 15
IS was spiked to all samples. After reviewing signal of all of them, we selected
two to guide this analysis process. However, we also advise to plot all the ions
after each steps.

Below, we generate Extracted Ion Chromatograms (EIC) for these selected *test
ions*. By restricting the MS data to intensities within a restricted, small
*m/z* range and a selected retention time window, EICs are expected to contain
only signal from a single type of ion. The expected *m/z* and retention times
for our set of IS was determined in a different experiment. Additionally, in
cases where internal standards are not available, commonly present ions in the
sample matrix can serve as suitable alternatives. Ideally, these compounds
should be distributed across the entire retention time range of the experiment.

```{r EIC extract for internal standard}
#' Load our list of standard
intern_standard <- read.delim("intern_standard_list.txt")

# Extract EICs for the list
eic_is <- chromatogram(
    data,
    rt = as.matrix(intern_standard[, c("rtmin", "rtmax")]),
    mz = as.matrix(intern_standard[, c("mzmin", "mzmax")]))

#' Add internal standard metadata
fData(eic_is)$mz <- intern_standard$mz
fData(eic_is)$rt <- intern_standard$RT
fData(eic_is)$name <- intern_standard$name
fData(eic_is)$abbreviation <- intern_standard$abbreviation
rownames(eic_is) <- intern_standard$abbreviation

#' Summary of IS information
cpt <- paste("Internal standard list with respective m/z and expected",
             "retention time [s].")
fData(eic_is)[, c("name", "mz", "rt")] |>
  as.data.frame() |>
  pandoc.table(style = "rmarkdown", caption = cpt, split.table = Inf)
```

We below plot the EICs for isotope labeled cystine and methionine.

```{r fig.height=4, fig.width=7}
#' Extract the two IS from the chromatogram object.
eic_cystine <- eic_is["cystine_13C_15N"]
eic_met <- eic_is["methionine_13C_15N"]

#' plot both EIC
par(mfrow = c(1, 2), mar = c(4, 2, 2, 0.5))
plot(eic_cystine, main = fData(eic_cystine)$name, cex.axis = 0.8,
     cex.main = 0.8,
     col = paste0(col_sample, 80))
grid()
abline(v = fData(eic_cystine)$rt, col = "red", lty = 3)

plot(eic_met, main = fData(eic_met)$name, cex.axis = 0.8, cex.main = 0.8,
     col = paste0(col_sample, 80))
grid()
abline(v = fData(eic_met)$rt, col = "red", lty = 3)
legend("topright", col = col_phenotype, legend = names(col_phenotype), lty = 1,
       bty = "n")
```

We can observe a clear concentration difference between QCs and study samples
for the isotope labeled cystine ion. However, the labeled methionine internal
standard exhibits a discernible signal amidst some noise and a noticeable
retention time shift between samples.

While the artificially isotope labeled compounds were spiked to the individual
samples, there should also be the signal from the endogenous compounds in serum
(or plasma) samples. We thus below calculate next the mass and *m/z* of an
*[M+H]+* ion of the endogenous cystine from its chemical formula and extract
also the EIC from this ion. For calculation of the exact mass and the *m/z* of
the selected ion adduct we use the `calculateMass()` and `mass2mz()` functions
from the `r Biocpkg("MetaboCoreUtils")` package.

```{r}
#' extract endogenous cystine mass and EIC and plot.
cysmass <- calculateMass("C6H12N2O4S2")
cys_endo <- mass2mz(cysmass, adduct = "[M+H]+")[, 1]

#' Plot versus spiked
par(mfrow = c(1, 2))
chromatogram(data, mz = cys_endo + c(-0.005, 0.005),
             rt = unlist(fData(eic_cystine)[, c("rtmin", "rtmax")]),
             aggregationFun = "max") |>
    plot(col = paste0(col_sample, 80)) |>
    grid()

plot(eic_cystine, col = paste0(col_sample, 80))
grid()
legend("topright", col = col_phenotype, legend = names(col_phenotype), lty = 1,
       bty = "n")
```

The two cystine EICs above look highly similar (the endogenous shown left, the
isotope labeled right in the plot above), if not for the shift in m/z, which
arises from the artificial labeling. This shift allows us to discriminate
between the endogenous and non-endogenous compound.


# Data preprocessing

Preprocessing stands as the inaugural step in the analysis of untargeted LC-MS
or gas chromatography (GC)-MS data. The primary objective of preprocessing is
the quantification of signals from ions measured in a sample, addressing any
potential retention time drifts between samples, and ensuring alignment of
quantified signals across samples within an experiment. The final result of
LC-MS data preprocessing is a numeric matrix with abundances of quantified
entities in the samples of the experiment.

## Chromatographic peak detection

The initial preprocessing step involves detecting of intensity peaks along the
retention time axis, the so called *chromatographic peaks*. To achieve this, we
employ the `findChromPeaks()` function within *xcms*. This function supports
various algorithms for peak detection, which can be selected and configured with
their respective parameter objects, with notable options including:

- `MatchedFilterParam`: Implements peak detection as described in the original
  xcms article [@smith_xcms:_2006].

- `CentWaveParam`: Utilizes continuous wavelet transformation (CWT)-based peak
  detection [@tautenhahn_highly_2008].

- `MassifquantParam`: Employs a Kalman filter-based peak detection
  [@conley_massifquant:_2014].

The preferred algorithm, in this case, is *CentWave*, known for its
effectiveness in handling non-Gaussian shaped chromatographic peaks or peaks
with different retention time widths, commonly encountered in HILIC
separation.

Below we apply the CentWave algorithm with its default settings on the extracted
ion chromatogram for cystine and methionine ions and evaluate its results.

```{r Default centwave param test}
#' Use default Centwave parameter
param <- CentWaveParam()

#' Evaluate for Cystine
cystine_test <- findChromPeaks(eic_cystine, param = param)
chromPeaks(cystine_test)

#' Evaluate for Methionine
met_test <- findChromPeaks(eic_met, param = param)
chromPeaks(met_test)
```

While *CentWave* is a highly performant algorithm, it necessitates adaptation to
each dataset. This implies that the parameters should be fine-tuned based on the
user's data. The example above serves as a clear motivation for users to
familiarize themselves with the various parameters and the need to adapt them to
a data set. We will discuss the main parameters that can be easily adjusted to
suit the user's dataset:

- `peakwidth`: Specifies the minimal and maximal expected width of the peaks in
  the retention time dimension. Highly dependent on the chromatographic settings
  used.

- `ppm`: The maximal allowed difference of mass peaks' *m/z* values (in
  parts-per-million) in consecutive scans to consider them representing signal
  from the same ion.

- `integrate`: This parameter defines the integration method. Here, we primarily
  use `integrate = 2` because it integrates also signal of a chromatographic
  peak's tail and is considered more accurate by the developers.

To determine `peakwidth`, we recommend that users refer to previous EICs and
estimate the range of peak widths they observe in their dataset. Ideally,
examining multiple EICs should be the goal. For this dataset, the peak widths
appear to be around 2 to 10 seconds. We do not advise on going too wide
or too narrow with the `peakwidth` parameter as it can lead to false positives
or negatives.

To determine the `ppm`, a deeper analysis of the dataset is needed. It is
clarified above that `ppm` depends on the instrument, but users should not
necessarily input the vendor-advertised ppm. Here's how to determine it as
accurately as possible:

The following steps involve generating a highly restricted MS area with a single
mass peak per spectrum, representing the cystine ion. The *m/z* of these peaks
is then extracted, their absolute difference calculated and finally expressed in
ppm.

```{r ppm parameter1 }
#' Restrict the data to signal from cystine in the first sample
cst <- data[1L] |>
  spectra() |>
  filterRt(rt = c(208, 218)) |>
  filterMzRange(mz = fData(eic_cystine)["cystine_13C_15N", c("mzmin", "mzmax")])

#' Show the number of peaks per m/z filtered spectra
lengths(cst)

#' Calculate the difference in m/z values between scans
mz_diff <- cst |>
    mz() |>
    unlist() |>
    diff() |>
    abs()

#' Express differences in ppm
range(mz_diff * 1e6 / mean(unlist(mz(cst))))
```

We therefore, choose a value close to the maximum within this range for
parameter `ppm`.

Now, we perform the chromatographic peak detection process with the adapted
settings on our EICs. It is important to note that, to properly estimate
background noise, sufficient data points outside the chromatographic peak need
to be present. This is generally no problem if peak detection is performed on
the full LC-MS data set, but for peak detection on EICs the retention time range
of the EIC needs to be sufficiently wide. If the function fails to find a peak
in an EIC, the initial troubleshooting step should be to increase this
range. Also, the signal-to-noise threshold `snthresh` should in general be
reduced for peak detection in EICs, because within the small retention time
range usually not enough signal is present to properly estimate the background
noise. Finally, setting CentWave's advanced parameter `extendLengthMSW` to
`TRUE` can help if chromatographic peak detection fails despite choosing
apparently correct settings. [jo: maybe drop or reformulate that last sentence?]

```{r}
#' Parameters adapted for chromatographic peak detection on EICs.
param <- CentWaveParam(peakwidth = c(1, 8), ppm = 15, integrate = 2,
                       snthresh = 2)

#' Evaluate on the cystine ion
cystine_test <- findChromPeaks(eic_cystine, param = param)
chromPeaks(cystine_test)

#' Evaluate on the methionine ion
met_test <- findChromPeaks(eic_met, param = param)
chromPeaks(met_test)
```

With the customized parameters, a chromatographic peak was detected in each
sample. Below, we use the `plot()` function on the EICs to visualize these
results.

```{r echo=FALSE}
#' Plot test chromatogram
par(mfrow = c(1, 2))
plot(cystine_test, main = fData(cystine_test)$name,
     col = paste0(col_sample, 80),
     peakBg = paste0(col_sample, 40)[chromPeaks(cystine_test)[, "column"]],
     cex.main = 0.8, cex.axis = 0.8)
grid()

plot(met_test, main = fData(met_test)$name,
     col = paste0(col_sample, 80),
     peakBg = paste0(col_sample, 40)[chromPeaks(met_test)[, "column"]],
     cex.main = 0.8, cex.axis = 0.8)
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, bty = "n")
```

Our custom settings seem thus to be suitable for our dataset. We now proceed and
apply them to the entire dataset, extracting EICs again for the same ions to
evaluate and confirm that chromatographic peak detection worked as
expected. Note that we change the value for parameter `snthresh` to its default,
because, as mentioned above, background noise estimation is more reliable when
performed on the full data set. Parameter `chunkSize` of `findChromPeaks()`
defines the number of data files that are loaded into memory and processed
simultaneously. This parameter thus allows to fine-tune the memory demand as
well as performance of the chromatographic peak detection step.

```{r run find chrompeak on entire dataset}
#' Using the same settings, but with default snthresh
param <- CentWaveParam(peakwidth = c(1, 8), ppm = 15, integrate = 2)
data <- findChromPeaks(data, param = param, chunkSize = 5)

#' Update EIC internal standard object
eics_is_noprocess <- eic_is
eic_is <- chromatogram(data,
                       rt = as.matrix(intern_standard[, c("rtmin", "rtmax")]),
                       mz = as.matrix(intern_standard[, c("mzmin", "mzmax")]))
fData(eic_is) <- fData(eics_is_noprocess)
```

Below we plot the EICs of the two selected internal standards to evaluate the
chromatographic peak detection results.

```{r plot new eics, echo=FALSE}
#' Test if we find peaks for Cystine and Methionine again
eic_cystine <- eic_is["cystine_13C_15N", ]
eic_met <- eic_is["methionine_13C_15N", ]

#' Plot
par(mfrow = c(1, 2))
plot(eic_cystine, main = "L-Cystine (13C6, 99%; 15N2, 99%)",
     col = paste0(col_sample, 80),
     peakBg = paste0(col_sample[chromPeaks(eic_cystine)[, "sample"]], 40))
grid()

plot(eic_met, main = "L-Methionine (13C5, 99%; 15N, 99%)",
     col = paste0(col_sample, 80),
     peakBg = paste0(col_sample[chromPeaks(eic_met)[, "sample"]], 40))
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1)
```

Peaks seems to have been detected properly in all samples for both ions. This
indicates that the peak detection process was successful.


### Refine identified chromatographic peaks

The identification of chromatographic peaks using the *CentWave* algorithm can
sometimes result in artifacts, such as overlapping or split peaks. To address
this issue, the `refineChromPeaks()` function is utilized, in conjunction with
the `MergeNeighboringPeaksParam`, which aims at merging such split peaks.

Below we show some examples of *CentWave* peak detection artifacts. These
examples are pre-selected to illustrate the necessity of the next step:

```{r echo=FALSE}
#' Extract m/z-rt regions for selected peaks
mz_rt <- cbind(rtmin = c(155.2120, 181.71800),
               rtmax = c(201.0710, 228.13500),
               mzmin = c(191.0288,  53.50964),
               mzmax = c(191.0527, 53.53183))

#' Extract the EICs
eics <- chromatogram(data[3], rt = mz_rt[, c("rtmin", "rtmax")],
                     mz = mz_rt[, c("mzmin", "mzmax")])
#' Plot the EICs
plot(eics)
```

In both cases the signal presumably from a single type of ion was split into two
separate chromatographic peaks (indicated by the vertical line). The
`MergeNeigboringPeaksParam` allows to combine such split peaks. The parameters
for this algorithm are `expandMz`, `expandRt` and `minProp`. `expandMz` and
`expandRt` define which chromatographic peaks should be evaluated for merging
and `minProp` is used to determine whether candidates are actually merged:
chromatographic peaks with overlapping *m/z* ranges (expanded on each side by
`expandMz`) and with a distance tail to head in retention time dimension that is
less than `2 * expandRt` and for which the signal between them is higher than
`minProp` of the apex intensity of the chromatographic peak with the lower
intensity are merged.

Parameter `expandRt` is usually set to approximately half the size of the
average retention time width used for chromatographic peak detection (in this
case 2.5 seconds). Parameter `expandMz` is suggested to be kept relatively small
(here at 0.0015) to prevent the merging of isotopes. Also, values for `minProp`
should not be too small to avoid merging of closely co-eluting ions such as
isomers. We test these settings below on the EICs with the split peaks.

```{r test merging}
#' set up the parameter
param <- MergeNeighboringPeaksParam(expandRt = 2.5, expandMz = 0.0015,
                                    minProp = 0.75)

#' Perform the peak refinement on the EICs
eics <- refineChromPeaks(eics, param = param)
plot(eics)
```

We can observe that the artificially split peaks have been appropriately merged.
Therefore, we next apply these settings to the entire dataset. After peak
merging, column `"merged"` in the result object's `chromPeakData()` data frame
can be used to evaluate which chromatographic peaks in the result represent
signal from merged, or from the originally identified chromatographic peaks.

```{r}
#' Apply on whole dataset
data <- refineChromPeaks(data, param = param, chunkSize = 5)
chromPeakData(data)$merged |>
                      table()
```

Before proceeding with the next preprocessing step it is generally suggested to
evaluate the results of the chromatographic peak detection on EICs of
e.g. internal standards or other compounds/ions known to be present in the
samples.

```{r}
eics_is_chrompeaks <- eic_is

eic_is <- chromatogram(data,
                       rt = as.matrix(intern_standard[, c("rtmin", "rtmax")]),
                       mz = as.matrix(intern_standard[, c("mzmin", "mzmax")]))
fData(eic_is) <- fData(eics_is_chrompeaks)

eic_cystine <- eic_is["cystine_13C_15N", ]
eic_met <- eic_is["methionine_13C_15N", ]
```

Also evaluating and comparing the number of identified chromatographic peaks in
all samples of a data set can help spotting potentially problematic
samples. Below we count the number of chromatographic peaks per sample and show
these numbers in a table.

```{r, results = "asis"}
#' Count the number of peaks per sample and summarize them in a table.
data.frame(sample_name = sampleData(data)$sample_name,
           peak_count = as.integer(table(chromPeaks(data)[, "sample"]))) |>
    pandoc.table(
        style = "rmarkdown",
        caption = "Samples and number of identified chromatographic peaks.")
```

About the same number of chromatographic peaks was identified in the various
samples of the data set.

There would be additional options to evaluate chromatographic peak detection
results from visualizations such as those created with `plotChromPeaks()` to
summaries that could be easily generated with base R commands.


## Retention time alignment

[jo: I rewrote/reformulated most of the text here - I hope it's clear and
understandable. I tried to formulate it for analysts without any pre-knowledge
of xcms or LC-MS data analysis, also focusing more on the algorithms we use here
instead of mentioning what else would be available.]

Despite using the same chromatographic settings and conditions there will always
be shifts in retention times between samples from an experiment. These will be
generally small if samples are measured within the same batch/measurement run,
but can be considerable if data of an experiment was acquired across a longer
time period.

Below we extract base peak chromatograms from the study's QC samples and plot
them.

```{r echo=FALSE}
#' Get QC samples
QC_samples <- sampleData(data)$phenotype == "QC"

#' extract BPC
data[QC_samples] |>
    chromatogram(aggregationFun = "max", chromPeaks = "none") |>
    plot(col = col_phenotype["QC"], main = "BPC of QC samples") |>
    grid()
```

These QC samples representing the same sample (pool) were measured at regular
intervals during the measurement run of the experiment and were all measured on
the same day. Still, small shifts can be observed, especially between 100 and
150 seconds. To facilitate proper correspondence of signals across samples (and
hence definition of the LC-MS features), it is essential to minimize these
differences in retention times.

[jo: would maybe not discuss the different available algorithms, but just focus
on the method used here. Info on the other algorithms would be available in the
xcms tutorial and vignette.]

In *xcms* retention time alignment can be performed using the `adjustRtime()`
function and one of the available alignments algorithms that can be selected and
configured using the respective parameter object. For this example we use the
*PeakGroups* method [@smith_xcms:_2006] that performs the alignment by
minimizing differences in retention times of a set of *anchor peaks* in the
different samples. This method requires an initial correspondence analysis to
match/group chromatographic peaks across samples from which the algorithm then
selects the anchor peaks for the alignment.

For the initial correspondence, we use the *PeakDensity* approach
[smith_xcms:_2006] that groups chromatographic peaks with similar *m/z* and
retention time into LC-MS features. An LC-MS feature, or in short feature,
represents signal from the same type of ion across all samples of a data
set. The parameters for this algorithm, that can be configured using the
`PeakDensityParam` object, are `sampleGroups`, `minFraction`, `binSize`, `ppm`
and `bw`. `binSize`, `ppm` and `bw` allow to specify how similar the
chromatographic peaks' *m/z* and retention time values need to be to consider
them for grouping into a feature. Here `binSize` and `ppm` define the required
similarity of *m/z* values. Within each *m/z* bin (defined by `binSize` and
`ppm`) areas along the retention time axis with a high chromatographic peak
density (considering all peaks in all samples) are identified, and
chromatographic peaks within these regions are considered for grouping into a
feature. High density areas are identified using the base R `density()`
function, for which `bw` is a parameter: higher values define wider retention
time areas, lower values require chromatographic peaks to have more similar
retention times. Whether such candidate peaks get grouped into a feature depends
also on parameters `sampleGroups` and `minFraction`: `sampleGroups` should
provide, for each sample, the sample group it belongs to, and `minFraction` is
expected to be a value between 0 and 1 defining the proportion of samples within
at least one of the sample groups (defined with `sampleGroups`) in which a
chromatographic peaks was detected to group them into a feature.

For the initial correspondence, parameters don't need to be fully
optimized. Selection of data set-specific parameter values is described in more
detail in the following section. For our data set, we use small values for
`binSize` and `ppm` and, importantly, also for parameter `bw`, since for our
data set an ultra high performance (UHP) LC setup was used. For `minFraction` we
use a high value (0.9) to ensure only features are defined for chromatographic
peaks present in almost all samples of one sample group (which can then be used
as anchor peaks for the actual alignment). We will base the alignment later on
QC samples only and hence define for `sampleGroups` a binary variable grouping
samples either into a study, or QC group.

```{r}
# Initial correspondence analysis
param <- PeakDensityParam(sampleGroups = sampleData(data)$phenotype == "QC",
                          minFraction = 0.9,
                          binSize = 0.01, ppm = 10,
                          bw = 2)
data <- groupChromPeaks(data, param = param)
```

*PeakGroups*-based alignment can next be performed using the `adjustRtime()`
function with a `PeakGroupsParam` parameter object. The parameters for this
algorithm are:

- `subsetAdjust` and `subset`: Allows for subset alignment. Here we base the
  retention alignment on the QC samples, i.e., retention time shifts will be
  estimated based on these repeatedly measured samples. The resulting adjustment
  is then applied to the entire data. For data sets in which QC samples
  (e.g. sample pools) are measured repeatedly, we strongly suggest to use this
  method. Note also that for subset-based alignment the samples should be
  ordered by injection index (i.e., in the order in which they were measured
  during the measurement run).

- `minFraction`: a value between 0 and 1 defining the proportion of samples (of
  the full data set, or the data subset defined with `subset`) in which a
  chromatographic peak had to be identified to use it as *anchor peak*. This is
  in contrast to the `PeakDensityParam` where this parameter was used to define
  a proportion within a sample group.

- `span`: The *PeakGroups* method allows, depending on the data,
  to adjust regions along the retention time axis differently. To enable such
  local alignments the *LOESS* function is used and this parameter defines the
  degree of smoothing of this function. Generally, values between 0.4 and 0.6
  are used, however, it is suggested to evaluate alignment results and
  eventually adapt parameters if the result was not satisfactory.

Below we perform the alignment of our data set based on retention times of
anchor peaks defined in the subset of QC samples. QC samples, e.g. a pool of the
study samples, repeatedly measured within an experiment are ideal for alignment,
because the same ions are expected to be quantified in all of them enabling
easier and more reliable definition of anchor peaks. Despite having external QCs
in our data set, we still use the subset-based alignment assuming retention time
shifts to be independent of the different sample matrix (human serum or plasma).

```{r}
#' Define parameters of choice
subset <- which(sampleData(data)$phenotype == "QC")
param <- PeakGroupsParam(minFraction = 0.9, extraPeaks = 50, span = 0.5,
                         subsetAdjust = "average",
                         subset = subset)

#' Perform the alignment
data <- adjustRtime(data, param = param)
```

Alignment adjusted the retention times of all spectra in the data set, as well
as the retention times of all identified chromatographic peaks. Note that it
would also be possible to manually specify anchor peaks, respectively their
retention times or to align a data set against an external, reference, data
set. More information is provided in the vignettes of the `r Biocpkg("xcms")`
package.

Once the alignment has been performed, the user should evaluate the results
using the `plotAdjustedRtime()` function. This function visualizes the
difference between adjusted and raw retention time for each sample on the y-axis
along the adjusted retention time on the x-axis. Dot points represent the
position of the used anchor peak along the retention time axis. For optimal
alignment in all areas along the retention time axis, these anchor peaks should
be scattered all over the retention time dimension.

```{r}
#' Visualize alignment results
plotAdjustedRtime(data, col = paste0(col_sample, 80), peakGroupsPch = 1)
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, bty = "n")
```

All samples from the present data set were measured within the same measurement
run, thus only small retention time shifts were present. Therefore, only little
adjustments needed to be performed (shifts of at maximum 1 second as can be seen
in the plot above). Generally, the magnitude of adjustment seen in such plots
should match the expectation from the analyst.

We can also compare the BPC before and after alignment. To get the original
data, i.e. the raw retention times, we can use the `dropAdjustedRtime()`
function:

```{r}
#' Get data before alignment
data_raw <- dropAdjustedRtime(data)

#' Apply the adjusted retention time to our dataset
data <- applyAdjustedRtime(data)
```

```{r bpc before and after1}
#' Plot the BPC before and after alignment
par(mfrow = c(2, 1), mar = c(2, 1, 1, 0.5))
chromatogram(data_raw, aggregationFun = "max", chromPeaks = "none") |>
    plot(main = "BPC before alignment", col = paste0(col_sample, 80))
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, bty = "n", horiz = TRUE)

chromatogram(data, aggregationFun = "max", chromPeaks = "none") |>
    plot(main = "BPC after alignment",
         col = paste0(col_sample, 80))
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, bty = "n", horiz = TRUE)
```

The largest shift can be observed in the retention time range from 120 to 130s.
Apart from that retention time range, only little changes can be observed.

We next evaluate the impact of the alignment on the EICs of the selected
internal standards. We thus below first extract the ion chromatograms after
alignment.

```{r}
#' Store the EICs before alignment
eics_is_refined <- eic_is

#' Update the EICs
eic_is <- chromatogram(data,
                       rt = as.matrix(intern_standard[, c("rtmin", "rtmax")]),
                       mz = as.matrix(intern_standard[, c("mzmin", "mzmax")]))
fData(eic_is) <- fData(eics_is_refined)

#' Extract the EICs for the test ions
eic_cystine <- eic_is["cystine_13C_15N"]
eic_met <- eic_is["methionine_13C_15N"]
```

We can now evaluate the alignment effect in our test ions. We will plot the EICs
before and after alignment for both the isotope labeled cystine and methionine.

```{r specific ion before and after1}
par(mfrow = c(2, 2), mar = c(4, 4.5, 2, 1))

old_eic_cystine <- eics_is_refined["cystine_13C_15N"]
plot(old_eic_cystine, main = "Cystine before alignment", peakType = "none",
     col = paste0(col_sample, 80))
grid()
abline(v = intern_standard["cystine_13C_15N", "RT"], col = "red", lty = 3)

old_eic_met <- eics_is_refined["methionine_13C_15N"]
plot(old_eic_met, main = "Methionine before alignment",
     peakType = "none", col = paste0(col_sample, 80))
grid()
abline(v = intern_standard["methionine_13C_15N", "RT"], col = "red", lty = 3)

plot(eic_cystine, main = "Cystine after alignment", peakType = "none",
     col = paste0(col_sample, 80))
grid()
abline(v = intern_standard["cystine_13C_15N", "RT"], col = "red", lty = 3)
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, bty = "n")

plot(eic_met, main = "Methionine after alignment",
     peakType = "none", col = paste0(col_sample, 80))
grid()
abline(v = intern_standard["methionine_13C_15N", "RT"], col = "red", lty = 3)
```

The non_endogenous cystine ion was already well aligned so the difference is
minimal. The methionine ion, however, shows an improvement in alignment.

In addition to a visual inspection of the results, we next also evaluate the
impact of the alignment by comparing the variance in retention times for
internal standards before and after alignment. To this end we need to first
identify chromatographic peaks in each sample with an *m/z* and retention time
close to the expected values for each internal standard. For this we use the
`matchValues()` function from the `r Biocpkg("MetaboAnnotation")` package using
the `MzRtParam` method to identify all chromatographic peaks with similar *m/z*
(+/- 50ppm) and retention time (+/- 10 seconds) to the internal standard's
values. We perform this matching separate for each sample and select for each
internal standard, in each sample, using the `filterMatches()` and
`SingleMatchParam`, the chromatographic peak with the highest intensity.

```{r}
#' Extract the matrix with all chromatographic peaks and add a column
#' with the ID of the chromatographic peak
chrom_peaks <- chromPeaks(data) |> as.data.frame()
chrom_peaks$peak_id <- rownames(chrom_peaks)

#' Define the parameters for the matching and filtering of the matches
p_1 <- MzRtParam(ppm = 50, toleranceRt = 10)
p_2 <- SingleMatchParam(duplicates = "top_ranked", column = "target_maxo",
                        decreasing = TRUE)

#' Iterate over samples and identify for each the chromatographic peaks
#' with similar m/z and retention time than the onse from the internal
#' standard, and extract among them the ID of the peaks with the
#' highest intensity.
intern_standard_peaks <- lapply(seq_along(data), function(i) {
    tmp <- chrom_peaks[chrom_peaks[, "sample"] == i, , drop = FALSE]
    mtch <- matchValues(intern_standard, tmp,
                        mzColname = c("mz", "mz"),
                        rtColname = c("RT", "rt"),
                        param = p_1)
    mtch <- filterMatches(mtch, p_2)
    mtch$target_peak_id
}) |>
    do.call(what = cbind)

```

We have now for each internal standard the ID of the chromatographic peak in
each sample that most likely represents signal from that ion. We can now extract
the retention times for these chromatographic peaks before and after alignment.

```{r}
#' Define the index of the selected chromatographic peaks in the
#' full chromPeaks matrix
idx <- match(intern_standard_peaks, rownames(chromPeaks(data)))

#' Extract the raw retention times for these
rt_raw <- chromPeaks(data_raw)[idx, "rt"] |>
    matrix(ncol = length(data_raw))

#' Extract the adjusted retention times for these
rt_adj <- chromPeaks(data)[idx, "rt"] |>
    matrix(ncol = length(data_raw))

```

We can now evaluate the impact of the alignment on the retention times of
internal standards across the full data set

```{r}
list(all_raw = rowSds(rt_raw, na.rm = TRUE),
     all_adj = rowSds(rt_adj, na.rm = TRUE)
     ) |>
    vioplot(ylab = "sd(retention time)")
grid()
```

On average, the variation in retention times of internal standards across
samples was slightly reduced by the alignment.


## Correspondence

We briefly touched on the subject of correspondence before to determine anchor
peaks. Generally, the goal of the correspondence analysis is to identify
chromatographic peaks that originate from the same types of ions in all samples
of an experiment and to group them into LC-MS *features*. In *xcms*, this step
can be performed with the `groupChromPeaks()` function and one of the
correspondence algorithms that can be selected and configured with their
respective parameter object. In this example, we use the *PeakDensity* method
[[smith_xcms:_2006]], that was already described in the previous section. Next
to the `binSize` and `ppm` parameters, proper configuration of parameter `bw` is
crucial. Here we illustrate how sensible choices for this parameter's value can
be made. We use below the `plotChromPeakDensity()` function to *simulate* a
correspondence analysis with the default values for *PeakGroups* on the
extracted ion chromatograms of our two selected isotope labeled ions. This plot
shows the EIC in the top panel, and the apex position of chromatographic peaks
in the different samples (y-axis), along retention time (x-axis) in the lower
panel.

```{r}
#' Default parameter for the grouping and apply them to the test ions BPC
param <- PeakDensityParam(sampleGroups = sampleData(data)$phenotype, bw = 30)

plotChromPeakDensity(
    eic_cystine, param = param,
    col = paste0(col_sample, "80"),
    peakCol = col_sample[chromPeaks(eic_cystine)[, "sample"]],
    peakBg = paste0(col_sample[chromPeaks(eic_cystine)[, "sample"]], 20),
    peakPch = 16)
```

```{r}
plotChromPeakDensity(eic_met, param = param,
    col = paste0(col_sample, "80"),
    peakCol = col_sample[chromPeaks(eic_met)[, "sample"]],
    peakBg = paste0(col_sample[chromPeaks(eic_met)[, "sample"]], 20),
    peakPch = 16)
```

For this method, it's crucial to understand that grouping depends on the
smoothness of the density curve (shown as a black line in the lower panel of the
plot) that can be configured with the parameter `bw`.  As seen above, the
smoothness is too high to properly group our features. When looking at the
default parameters, we can observe that indeed, the `bw` parameter is set to `bw
= 30`, which is too high for modern UHPLC-MS setups. We reduce the value of this
parameter to 1.8 and evaluate its impact.

```{r}
#' Updating parameters
param <- PeakDensityParam(sampleGroups = sampleData(data)$phenotype, bw = 1.8)

plotChromPeakDensity(
    eic_cystine, param = param,
    col = paste0(col_sample, "80"),
    peakCol = col_sample[chromPeaks(eic_cystine)[, "sample"]],
    peakBg = paste0(col_sample[chromPeaks(eic_cystine)[, "sample"]], 20),
    peakPch = 16)
plotChromPeakDensity(eic_met, param = param,
    col = paste0(col_sample, "80"),
    peakCol = col_sample[chromPeaks(eic_met)[, "sample"]],
    peakBg = paste0(col_sample[chromPeaks(eic_met)[, "sample"]], 20),
    peakPch = 16)
```

We can observe that the peaks are now grouped more accurately into a single
feature for each test ion. The other important parameters are `binSize` and
`ppm` which allow to configure the required similarity of *m/z* values of the
chromatographic peaks to consider them for grouping. Our data was generated on a
high resolution MS instrument and we thus select a low value for
`binSize`. Also, for TOF instruments it is suggested to use a value for `ppm`
that is larger than 0 to accommodate the higher measurement error of the
instrument for larger *m/z* values. Finally, we set `minFraction = 0.75` hence
defining features only if in at least 75% of samples of one of the sample groups
a chromatographic peak was identified. For sample groups we use the information
available in our `sampleData`'` `"phenotype"` column.

```{r}
#' Define the settings for the.
param <- PeakDensityParam(sampleGroups = sampleData(data)$phenotype,
                          minFraction = 0.75, binSize = 0.01, ppm = 10,
                          bw = 1.8)

#' Apply to whole data
data <- groupChromPeaks(data, param = param)
```

After correspondence analysis it is suggested to evaluate the results again for
selected EICs. Below we extract signal for an *m/z* similar to that of the
isotope labeled methionine for a larger retention time range. Importantly, to
show the actual correspondence results, we set `simulate = FALSE` for the
`plotChromPeakDensity()` function.

```{r}
#' Extract chromatogram for an m/z similar to the one of the labeled methionine
chr_test <- chromatogram(data,
                         mz = as.matrix(intern_standard["methionine_13C_15N",
                                                        c("mzmin", "mzmax")]),
                         rt = c(145, 200),
                         aggregationFun = "max")
plotChromPeakDensity(
    chr_test, simulate = FALSE,
    col = paste0(col_sample, "80"),
    peakCol = col_sample[chromPeaks(chr_test)[, "sample"]],
    peakBg = paste0(col_sample[chromPeaks(chr_test)[, "sample"]], 20),
    peakPch = 16)
```

Indeed, signal from the two different ions were now grouped into separate
features. Generally, correspondence results should be evaluated on more such
extracted chromatograms.

The results from the correspondence analysis are now stored, along with the
results from the other preprocessing steps, within our `XcmsExperiment` result
object. The correspondence results, i.e., the definition of the LC-MS features,
can be extracted using the `featureDefinitions()` function.

```{r}
#' Definition of the features
featureDefinitions(data) |>
  head()
```

This data frame provides the average *m/z* and retention time (in columns
`"mzmed"` and `"rtmed"`) that characterize a LC-MS feature. Column, `"peakidx"`
contains the indices of all chromatographic peaks that were assigned to that
feature. The actual abundances for these features, which represent also the
final preprocessing results, can be extracted with the `featureValues()`
function:

```{r}
#' Extract feature abundances
featureValues(data, method = "sum") |>
    head()
```

We can note that some features (e.g. F00003 and F00006) have missing values in
some samples. This is expected to a certain degree as not in all samples
features, respectively their ions, need to be present. We will address this in
the next section.


## Gap filling

The previously observed missing values (*NA*) could be attributed to various
reasons. Even if they represent a genuinely missing value, indicating that an
ion (feature) is truly not present in a particular sample, it could also be a
result of a failure in the preceding chromatographic peak detection step. It is
crucial to be able to recover missing values of the latter category as much as
possible to reduce the eventual need for data imputation. We next examine how
prevalent missing values are in our present dataset:

```{r}
#' Percentage of missing values
sum(is.na(featureValues(data))) /
    length(featureValues(data)) * 100
```

We can observe a substantial number of missing values values in our dataset.

Now, let's delve into the process of *gap-filling*. We first evaluate some
example features for which a chromatographic peak was only detected in some
samples:

```{r}
ftidx <- which(is.na(rowSums(featureValues(data))))
fts <- rownames(featureDefinitions(data))[ftidx]
farea <- featureArea(data, features = fts[1:2])

chromatogram(data[c(2, 3)],
             rt = farea[, c("rtmin", "rtmax")],
             mz = farea[, c("mzmin", "mzmax")]) |>
    plot(col = c("red", "blue"), lwd = 2)
```

In both instances, a chromatographic peak was only identified in one of the two
selected samples (red line), which no peak was detected in the blue labeled
sample and hence a missing value is reported for this feature in that particular
samples. Also, in both cases, signal was measured in both samples, thus,
reporting a missing values would not be correct in this case. The signal for
this feature is, particularly in the blue labeled sample, is very low and
sketchy. That is also the most likely reason peak detection failed. To rescue
signal in such cases, the `fillChromPeaks()` function can be used with the
`ChromPeakAreaParam` approach. This method defines for each feature, based on
the detected peaks, the *m/z* - retention time area in which signal for the
respective ion is expected, and integrates, in samples with missing values for
that feature, all intensities within the area. This is then reported as feature
abundance. Below we apply this method using the default parameters.

```{r}
#' Fill in the missing values in the whole dataset
data <- fillChromPeaks(data, param = ChromPeakAreaParam(), chunkSize = 5)

#' Percentage of missing values after gap-filling
sum(is.na(featureValues(data))) /
    length(featureValues(data)) * 100
```

With `fillChromPeaks()` we could thus rescue most of the missing data in the
data set. Note that, even if in a sample no ion would be present, in the worst
case noise would be integrated, which is expected to be much lower than actual
chromatographic peak signal. Let's look at our previously missing values again:

```{r echo=FALSE}
#' Extract EICs again and plot them
chromatogram(data[c(2, 3)],
             mz = farea[, c("mzmin", "mzmax")],
             rt = farea[, c("rtmin", "rtmax")]) |>
    plot(col = c("red", "blue"), lwd = 2)
```

After gap-filling, also in the blue colored sample a chromatographic peak is
present and its peak area would be reported as feature abundance for that
sample.

To further assess the effectiveness of the gap-filling method for rescuing
signals, we can also plot the average of features with at least one missing
value against the average filled-in signal. It is advisable to perform this
analysis on repeatedly measured samples; in this case, our QC/POOL samples will
be used.

For this, we extract:

- Feature values from detected chromatographic peaks by setting `filled = FALSE`
  in the `featuresValues()` call.

- The filled-in signal by first extracting both detected and gap-filled
  abundances and then replace the values for detected chromatographic peaks with
  `NA`.

Then, we calculate the row averages of both of these matrices and plot them
against each other.

```{r Detected vs filled signal1}
#' Get only detected signal in QC samples
vals_detect <- featureValues(data, filled = FALSE)[, QC_samples]

#' Get detected and filled-in signal
vals_filled <- featureValues(data)[, QC_samples]

#' Replace detected signal with NA
vals_filled[!is.na(vals_detect)] <- NA

#' Identify features with at least one filled peak
has_filled <- is.na(rowSums(vals_detect))

#' Calculate row averages for features with missing values
avg_detect <- rowMeans(vals_detect[has_filled, ], na.rm = TRUE)
avg_filled <- rowMeans(vals_filled[has_filled, ], na.rm = TRUE)

#' Plot the values against each other (in log2 scale)
plot(log2(avg_detect), log2(avg_filled),
     xlim = range(log2(c(avg_detect, avg_filled)), na.rm = TRUE),
     ylim = range(log2(c(avg_detect, avg_filled)), na.rm = TRUE),
     pch = 21, bg = "#00000020", col = "#00000080")
grid()
abline(0, 1)

```

The detected (x-axis) and gap-filled (y-axis) values for QC samples are highly
correlated. Especially for higher abundances, the agreement is very high, while
for low intensities, as can be expected, differences are higher. Below we in
addition fit a linear regression line to the data and summarize its results

```{r}
#' fit a linear regression line to the data
l <- lm(log2(avg_filled) ~ log2(avg_detect))
summary(l)
```

The linear regression line has a slope of `r round(coef(l)[2], 2)` and an
intercept of `r round(coef(l)[1], 2)`. This indicates that the filled-in signal
is on average `r round(coef(l)[2], 2)` times higher than the detected signal.


## Preprocessing result

The final results of the LC-MS data preprocessing are stored within the
`XcmsExperiment` object. This includes the identified chromatographic peaks, the
alignment results, as well as the correspondence results. In addition, to
guarantee reproducibility, this result object keeps track of all performed
processing steps, including the individual parameter objects used to configure
these. The `processHistory()` function returns a list of the various applied
processing steps in chronological order. Below we extract the information for
the first step of the performed preprocessing.

```{r Process history}
#' Check first step of the process history
processHistory(data)[[1]]
```

The `processParam()` function could then be used to extract the actual parameter
class used to configure this processing step.

The final result of the whole LC-MS data preprocessing would be a
two-dimensional matrix with abundances of the so-called LC-MS features in all
samples. Note that at this stage of the analysis features are only characterized
by their *m/z* and retention time and we don't have yet any information which
metabolite a feature could represent.

As we have seen before, such feature matrix could be extracted with the
`featureValues()` function and the corresponding feature characteristics (i.e.,
their *m/z* and retention time values) using the `featureDefinitions()`
function. Thus, these two arrays could be extracted from the *xcms* result
object and used/imported in other analysis packages for further processing.
They could for example also be exported to tab delimited text files, and used in
an external tool, or used, if also MS2 spectra would be available for
feature-based molecular networking in the GNPS analysis environment
[@nothias_feature-based_2020] (see also the [GNPS
documentation](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-xcms3/)
for more information).

For further processing in R, if no reference or link to the raw MS data is
required, it is however suggested to extract the *xcms* preprocessing result
using the `quantify()` function as a `SummarizedExperiment` object,
Bioconductor's default container for data from biological
assays/experiments. This simplifies integration with other Bioconductor analysis
packages. The `quantify()` function takes the same parameters than the
`featureValues()` function, thus, with the call below we extract a
`SummarizedExperiment` with only the detected, but not gap-filled, feature
abundances:

```{r}
#' Extract results as a SummarizedExperiment
res <- quantify(data, method = "sum", filled = FALSE)
res

```

Sample annotations from the *xcms* result's `sampleData()` are now available as
`colData()` (column, sample annotations) and the `featureDefinitions()` as
`rowData()` (row, feature annotations). The feature values have been added as
the first `assay()` in the `SummarizedExperiment` and even the processing
history is available in the object's `metadata()`. A `SummarizedExperiment`
supports multiple assays, all being numeric matrices with the same
dimensions. Below we thus add the detected and gap-filled feature abundances as
an additional assay to the `SummarizedExperiment`.

```{r}
assays(res)$raw_filled <- featureValues(data, method = "sum",
                                        filled = TRUE )

#' Different assay in the SummarizedExperiment object
assayNames(res)
```

Feature abundances can be extracted with the `assay()` function. Below we
extract the first 6 lines of the detected and gap-filled feature abundances:

```{r}
assay(res, "raw_filled") |> head()
```

An advantage, in addition to being a container for the full preprocessing
results is also the possibility of an easy and intuitive creation of data
subsets ensuring data integrity. It would for example be very easy to subset the
full data to a selection of features and/or samples:

```{r}
res[1:14, 3:8]
```

The `XcmsExperiment` object can also be saved for later use using the
`storeResults()` function. The data can be exported in different formats, to
enable also easier integration with non-R-based software. Currently, it is
possible to export the data in R-specific *RData* format or as (separate) plain
text files. Export to the community developed open mzTab-M format is currently
being developed and will be supported in future. Below we export the *xcms*
result object with R's default binary format for object serialization.

```{r}
#' Save `XcmsExperiment` object
storeResults(data, RDataParam(fileName = "data.RData"))

#' Save `SummarizedExperiment` object
save(res, file = "SumExp.RData")
```


# Data normalization

After preprocessing, data normalization or scaling might needed to be applied to
remove any technical variances from the data. While simple approaches like
median scaling can be implemented with a few lines of R code, more advanced
normalization algorithms are available in packages such as Bioconductor's
*preprocessCore*.

Unwanted variation can arise from various sources and is highly dependent on the
experiment. Therefore, data normalization should be chosen carefully based on
experimental design, statistical aims, and the balance of accuracy and precision
achieved through the use of auxiliary information.

Sample preparation biases could be evaluated using internal standards, depending
however also when they were added to the sample mixes during sample
processing. Repeated measurements of QC samples (usually a pool of the study
samples) on the other hand allows to estimate and correct for LC-MS specific
biases.

jo: different types of noise to account for. Mention between sample differences,
general signal drifts and batch effects. Here we focus on only between sample
differences, for large experiments the other two would be more important. Refer
to the *MetaboCoreUtils* vignette/functionality for adjustment using linear
models.

## Initial quality assessment

A principal component analysis (PCA) is a very helpful tool for an initial,
unsupervised, quality assessment. In order to apply a PCA to the measured
feature abundances, we need however to impute missing values. We assume that
most missing values (after the gap-filling step) represent signal which is below
detection limit. In such cases, missing values can be replaced with random
values from the uniform distribution sampling numbers from half of smallest
measured value to the smallest measured value for a specific feature. The
uniform distribution is defined with two parameters (minimum and maximum) and
all values between them have an equal probability of being selected.

Below we impute missing values with that approach and add the resulting data
matrix as a new *assay* to our result object.

```{r}
#' Load preprocessing results
load("SumExp.RData")
loadResults(RDataParam("data.RData"))

#' Impute missing values using uniform distribution
na.unidis <- function(z) {
    na <- is.na(z)
    if (any(na)) {
        min = min(z, na.rm = TRUE)
        z[na] <- runif(sum(na), min = min/2, max = min)
    }
    z
}

#' Create an assay with the filled and imputed value.
tmp <- apply(assay(res, "raw_filled"), MARGIN = 1, na.unidis)
assays(res)$raw_filled_imputed <- t(tmp)
```

### PCA unsupervised

PCA is a powerful tool to detect biases in the data. It is a dimensionality
reduction technique that allows to visualize the data in a lower-dimensional
space. In the context of LC-MS data, PCA can be used to look for overall biases
in batch, sample, injection index, ect, ... It is important to note that PCA is
a linear method and might not be able to detect all biases in the data.

Before plotting the PCA, we will log2 transform the data, center and
scale the data. The log2 transformation is applied to stabilize the variance and
remove dependency on absolute abundances.

```{r unsupervised checks1, fig.height=4, fig.width=4, include=TRUE}
#' Log2 transform and scale data
vals <- assay(res, "raw_filled_imputed") |>
    log2() |>
    t() |>
    scale(center = TRUE, scale = TRUE)

pca_res <- prcomp(vals, scale = FALSE, center = FALSE)

# Plot by phenotype - find way to plot side by side.
vals_st <- cbind(vals, phenotype = res$phenotype)
autoplot(pca_res, data = vals_st , colour = 'phenotype', scale = 0) +
    scale_color_manual(values = col_phenotype)
autoplot(pca_res, data = vals_st, colour = 'phenotype', x = 3, y = 4, scale = 0) +
    scale_color_manual(values = col_phenotype)
```

The PCA above shows a clear separation of the study samples (plasma) from the QC
samples (serum) on the first principal component (PC1). The separation based on
phenotype is visible on the third principal component (PC3).

In some cases, it can be a better option to remove the imputed values and
evaluate the PCA again. This is especially true if the imputed values are
replacing a large proportion of the data.

### Intensity evaluation

Another important aspect to consider is the intensity of the features. The
intensity of the features can be evaluated by plotting the distribution of the
log2 transformed feature abundances. Below we will show the distribution of the
log2 transformed abundances for the raw and filled data.

```{r counts1, fig.height=7, fig.width=5, include=TRUE}
layout(mat = matrix(1:3, ncol = 1), height = c(0.2, 0.2, 0.8))

par(mar = c(0.2, 4.5, 0.2, 3))
barplot(apply(assay(res, "raw"), MARGIN = 2, function(x) sum(!is.na(x))),
        col = col_sample, ylab = "features raw data", xaxt = "n",
        space = 0.012)
barplot(apply(assay(res, "raw_filled"), MARGIN = 2, function(x) sum(!is.na(x))),
        col = col_sample, ylab = "features filled data", xaxt = "n",
        space = 0.012)
boxplot(log2(assay(res, "raw_filled")), xaxt = "n",
        ylab = expression(log[2]~abundance~filled~data),
        col = col_sample, outline=FALSE, medlty = "blank", border = col_sample,
        boxwex = 0.99 )
points(colMedians(log2(assay(res, "raw_filled")), na.rm = TRUE), type = "b",
       pch = 16)
grid(nx = NA, ny = NULL)
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty=1, lwd = 2, xpd = TRUE, ncol = 3,
       cex = 0.8,  bty = "n")
```

The superior part of the plot show that the gap filling steps allowed to rescue
a consequent number of *NAs* and allowed us to have a more equivalent number of
features per sample. As we assume that every sample should have a similar
amount of features detected. Additionally we observe that, on average, the
signal distribution from the individual samples is very similar.

Another way to evaluate the distribution of value within our data are relative
log abundance plots (RLA). Within group RLA assess the tightness of replicate
within groups and should have a median close to zero and low variation around
it. When used across groups, they allow to compare behavior between groups.
Below we will perform the former to show relative abundance of each sample to
the other sample of the same group by setting `group = res$phenotype`.

```{r rla-plot raw and filled1, fig.cap = "RLA plot for the raw data and filled data. Note: outliers are not drawn."}
par(mfrow = c(1, 1), mar = c(0.2, 4.5, 2.5, 3))
boxplot(rowRla(assay(res, "raw_filled"), group = res$phenotype),
        cex = 0.5, pch = 16,
        col = col_sample, ylab = "RLA",
        border = paste0(col_sample, 40), boxwex = 1,
        outline = FALSE, xaxt = "n", main = "Relative log abundance",
        cex.main = 1)
grid(nx = NA, ny = NULL)
abline(h = 0, lty=3, lwd = 1, col = "black")
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty=1, lwd = 2, xpd = TRUE, ncol = 3,
       cex = 0.8,  bty = "n")
```

On the RLA plot above, we can observe that the medians for most samples are
indeed centered around 0. Exception are two of the *CVD* samples. But
normalization (hopefully) helps there.

### Internal standard

We want to select the internal standard compounds for which we have features
defined. For this, we first reuse the internal standards data from before.
Using the `matchValues()` function from the `MetaboAnnotation` package, we will
identify features matching m/z and RT of internal standards. With `mzColname`
and `rtColname` we specify the column names in query (our IS) and target
(features detected in the data set) that contain the m/z or RT values.

We then filter these *matches* to only keep IS that match with ONLY one feature
and remove the others.

```{r include=TRUE}
# Do we keep IS in normalisation ? Does not give much info... Would simplify a bit
#' Creating a column within our IS table
intern_standard$feature_id <- NA_character_

#' Identify features matching m/z and RT of internal standards.
fdef <- featureDefinitions(data)
fdef$feature_id <- rownames(fdef)
match_intern_standard <- matchValues(
    query = intern_standard,
    target = fdef,
    mzColname = c("mz", "mzmed"),
    rtColname = c("RT", "rtmed"),
    param = MzRtParam(ppm = 50, toleranceRt = 10))

#' Keep only matches with a 1:1 mapping standard to feature.
param <- SingleMatchParam(duplicates = "closest", column = "score_rt",
                          decreasing = TRUE)
match_intern_standard <- filterMatches(match_intern_standard, param)

intern_standard$feature_id <- match_intern_standard$target_feature_id
intern_standard <- intern_standard[!is.na(intern_standard$feature_id), ]

## Let's look at the internal standards
is_features <- featureChromatograms(
    data, features = intern_standard[c("methionine_13C_15N", "cystine_13C_15N"),
                                     "feature_id"], expandRt = 5)

par(mfrow = c(1, 2))
plot(is_features[1, ], col = col_sample,
     peakBg = paste0(col_sample[chromPeaks(is_features[1, ])[, "sample"]], 40),
     main = intern_standard["methionine_13C_15N", "name"])
abline(v = intern_standard["methionine_13C_15N", "RT"], lty = 2)
plot(is_features[2, ], col = col_sample,
     peakBg = paste0(col_sample[chromPeaks(is_features[2, ])[, "sample"]], 40),
     main = intern_standard["cystine_13C_15N", "name"])
abline(v = intern_standard["cystine_13C_15N", "RT"], lty = 2)
```

As anticipated, our two internal standards have effectively identified features.
These internal standards play a crucial role in guiding the normalization
process. Given the assumption that the samples were artificially spiked, we
possess a known ground truth—that the abundance or intensity of the internal
standard should be consistent. Consequently, normalization aims to minimize
variation between samples for the internal standard, reinforcing the
reliability of our analyses.
This list of IS will be used to assess the normalization process.

## Between sample normalisation

The previous RLA plot showed that the data had some biases that need to be
corrected. Therefore, we will implement between-sample normalization using
filled-in features. This process effectively mitigates variations influenced by
technical issues, such as differences in sample preparation and injection
methods.  In this instance, we will employ a commonly used technique known as
median scaling.

### Median scaling

This method involves computing the median for each sample, followed by
determining the median of these individual sample medians. This ensures
consistent median values for each sample throughout the entire data set.
Maintaining uniformity in the average total metabolite abundance across all
samples is crucial for effective implementation.

This process aims to establish a shared baseline for the central tendency of
metabolite abundance, mitigating the impact of sample-specific technical
variations. This approach fosters a more robust and comparable analysis of the
top features across the data set. The assumption is that normalizing based on
the median, known for its lower sensitivity to extreme values, enhances the
comparability of top features and ensures a consistent average abundance across
samples.

```{r}
#' Compute median and generate normalization factor
mdns <- apply(assay(res, "raw_filled"), MARGIN = 2,
              median, na.rm = TRUE )
nf_mdn <- mdns / median(mdns)

#' divide dataset by median of median and create a new assay.
assays(res)$norm <- sweep(assay(res, "raw_filled"), MARGIN = 2, nf_mdn, '/')
assays(res)$norm_imputed <- sweep(assay(res, "raw_filled_imputed"), MARGIN = 2,
                                  nf_mdn, '/')
```

The median scaling is calculated for both imputed and non-imputed data, with
each set stored separately within the `SummarizedExperiment` object. This
approach facilitates testing various normalization strategies while maintaining
a record of all processing steps undertaken, enabling easy regression to
previous stages if necessary.

## Assessing overall effectiveness of the normalization approach

It is crucial to evaluate the effectiveness of the normalization process. This
can be achieved by comparing the distribution of the log2 transformed feature
abundances before and after normalization. Additionally, the relative log
abundance (RLA) plots can be used to assess the tightness of replicates within
groups and compare the behavior between groups.

### PCA

```{r fig.width=7, include=TRUE}
#' Data before
vals_st <- cbind(vals, phenotype = res$phenotype)
autoplot(pca_res, data = vals_st , colour = 'phenotype', scale = 0) +
    scale_color_manual(values = col_phenotype)

#' Data after
vals_norm <- apply(assay(res, "norm"), MARGIN = 1, na.unidis) |>
    log2() |>
    scale(center = TRUE, scale = TRUE)

pca_res_norm <- prcomp(vals_norm, scale = FALSE, center = FALSE)
vals_st_norm <- cbind(vals_norm, phenotype = res$phenotype)
autoplot(pca_res, data = vals_st , colour = 'phenotype', scale = 0) +
    scale_color_manual(values = col_phenotype)
```

The PCA plots above show that the normalization process has not changed the
overall structure of the data. The separation between the study and QC samples
remains the same. This is an expected results as normalisation should not
correct for biological variance and only technical.

### RLA

```{r rla-plot after norm2, include = TRUE, fig.cap = "RLA plot before and after normalization. Note: outliers are not drawn.", fig.height= 7, fig.width=5.5}
par(mfrow = c(2, 1), mar = c(1, 4, 3, 1))

boxplot(rowRla(assay(res, "raw_filled"), group = res$phenotype),
        cex = 0.5, pch = 16, col = col_sample, ylab = "RLA",
        border = paste0(col_sample, 40), notch = TRUE, cex.main = 1,
        outline = FALSE, xaxt = "n", main = "Raw data", boxwex = 1)
grid(nx = NA, ny = NULL)
legend("topleft", inset = c(0, -0.2), col = col_phenotype,
       legend = names(col_phenotype), lty=1, lwd = 2, xpd = TRUE,
       ncol = 3, cex = 0.7, bty = "n")
abline(h = 0, lty=3, lwd = 1, col = "black")

boxplot(rowRla(assay(res, "norm"), group = res$phenotype),
        cex = 0.5, pch = 16,
        col = col_sample, ylab = "RLA",
        border = paste0(col_sample, 40), notch = TRUE, boxwex = 1,
        outline = FALSE, xaxt = "n", main = "After normalization", cex.main = 1)
grid(nx = NA, ny = NULL)
abline(h = 0, lty=3, lwd = 1, col = "black")
```

On the other hand, the RLA plot shows that the normalization process has
effectively centered the data around the median. The medians for all samples are
now closer to zero, indicating that the normalization process has been effective
in reducing the bias in the data.

### Coefficient of variation

Below we can see the coefficient of variation before and after normalization in
the different sample types as well as for the IS.

jo: boxplot/violin plot of internal standards per sample?

```{r include=TRUE, results = "asis"}
index_study <- res$phenotype %in% c("CTR", "CVD")
index_QC <- res$phenotype == "QC"

sample_res <- cbind(
    QC_Raw = rowRsd(assay(res, "raw_filled")[, index_QC],
                    na.rm = TRUE, mad = TRUE),
    QC_norm = rowRsd(assay(res, "norm")[, index_QC],
                     na.rm = TRUE, mad = TRUE),
    Study_Raw = rowRsd(assay(res, "raw_filled")[, index_study],
                       na.rm = TRUE, mad = TRUE),
    Study_norm = rowRsd(assay(res, "norm")[, index_study],
                        na.rm = TRUE, mad = TRUE),
    IS_Raw = rowRsd(assay(res, "raw_filled")[intern_standard$feature_id, ],
                    na.rm = TRUE, mad = TRUE),
    IS_norm = rowRsd(assay(res, "norm")[intern_standard$feature_id, ],
                     na.rm = TRUE, mad = TRUE)
)

#' Quantile
res_df <- data.frame(
    QC_raw = quantile(sample_res[, "QC_Raw"], na.rm = TRUE),
    QC_Norm = quantile(sample_res[, "QC_norm"], na.rm = TRUE),
    Study_raw = quantile(sample_res[, "Study_Raw"], na.rm = TRUE),
    Study_Norm = quantile(sample_res[, "Study_norm"], na.rm = TRUE),
    Intern_standard_raw = quantile(sample_res[, "IS_Raw"], na.rm = TRUE),
    Intern_standard_norm = quantile(sample_res[, "IS_norm"], na.rm = TRUE)
)
cpt <- paste0("Distribution of RSD values across samples for the raw and ",
              "normalized data.")
pandoc.table(res_df, style = "rmarkdown", caption = cpt)
```

The table above shows the distribution of the coefficient of variation
(RSD) for both raw and normalized data. As anticipated, the RSD values for the
quality controls (QCs), which reflect technical variance, are lower compared to
those for the study samples, which include both technical and biological
variance. Overall, minimal disparity exists between the raw and normalized data,
which is a positive indication that the normalization process hasn't introduced
bias into the dataset.

Additionally, it's important to highlight the very low between-sample
differences in the raw data. This absence suggests uniformity in sample
preparation and processing, as all samples were measured in the same run,
minimizing biases from MS instrumentation.

```{r}
save(data, file = "data_afternorm.RData")
save(res, file = "SumExp_afternorm.RData")
```

### Conclusion

The overall conclusion of the normalization process is that very little variance
was present from the beginning however the normalization process was able to
center the data around the median (as shown by the RLA plot).
Given the simplicity and limited size of our dataset example, we will conclude
the normalization process at this stage. For more intricate datasets with
diverse biases, a tailored approach would be devised. For instance, employing
the `limma` package to execute linear models can effectively address biases
stemming from batch effects or injection order.

# Quality control: Feature prefiltering

After normalizing our data we can now pre-filter to clean the data before
performing any statistical analysis. In general, pre-filtering of samples and
features is performed to remove outliers.

Below we make it so as to keep a copy of the unfiltered object.

```{r}
load("SumExp_afternorm.RData")
load("data_afternorm.RData")

#' Number of features before filtering
nrow(res)

#' keep unfiltered object
res_unfilt <- res
```

Here we will eliminate features that exhibit high variability in our dataset.
QC samples typically serve as a robust basis for cleansing  datasets of
systematic and random errors. However, in our dataset, since our QC pool
samples consist of other biological samples rather than pools of the samples
themselves, their utility for filtering is somewhat constrained. For a
comprehensive understanding of guidelines for data filtering in untargeted
metabolomic studies, please refer to [insert link to the paper PHILI].

We first restrict the data set to features for which a chromatographic peak was
detected in at least 2/3 of samples of at least one of the study samples groups.
This ensures the statistical tests performed later on the study samples being
performed on *reliable* signal. Also, with this filter we remove features that
were mostly detected in QC samples, but not the study samples. Such filter can
be performed with `filterFeatures` and the `PercentMissingFilter`
setting. Parameter `threshold` of this filter defines the maximal acceptable
percentage of samples with missing value(s) in at least in one of the sample
groups defined by parameter `f`. To consider detected chromatographic peaks per
sample, we apply the filter on the `"raw"` assay of our result object, that
contains an abundance value only for detected chromatographic peaks (prior
gap-filling). By replacing the `"QC"` sample group for parameter `f` we consider
only study samples. With `threshold = 40` we remove features for which no peak
was identified in 2 out of the 3 samples per sample group.

```{r}
#' Limit features to those with at least two detected peaks in one study group.
#' Setting the factor value for QC samples excludes QC samples from the
#' calculation.
f <- res$phenotype
f[f == "QC"] <- NA
f <- as.factor(f)
res <- filterFeatures(res, PercentMissingFilter(f = f, threshold = 40),
                      assay = "raw")
```

Following the guidelines stated above we decided to still use the QC samples
for pre-filtering, on the basis that they represent similar bio-fluids to our
study samples, and thus, we anticipate observing relatively similar metabolites.

We therefore evaluate the Dratio for all features in the data set. Using the
same function as above but this time with the `DratioFilter` parameter. More
filter exist for this function and we invite the user to explore them as to
decide what is best for their dataset.

```{r}
#' COmpute and filter based on the Dratio
filter_dratio <- DratioFilter(threshold = 0.4,
                              qcIndex = res$phenotype == "QC",
                              studyIndex = res$phenotype != "QC",
                              mad = TRUE)
res <- filterFeatures(res, filter = filter_dratio, assay = "norm_imputed")#

#' Phili: i think you advised that we used the dratio filter only, but i am
#'struggling to justify it in a proper way below, as we do expect a high degree
#' of variability between the QC and study samples in any case...
```

The Dratio filter is a powerful tool to identify features that exhibit high
variability in the data. By setting a threshold of 0.4, we remove features that
have a high degree of variability between the QC and study samples. This
filtering step ensures that only features with consistent abundance levels
across the samples are retained for further analysis.

Finally, we will evaluate the number of features left after the filtering steps
and calculate the percentage of features that were removed.

```{r}
#' Number of features after analysis
nrow(res)

#' Percentage left: end/beginning
nrow(res)/nrow(res_unfilt)*100
```

The dataset has been reduced from `r nrow(res_unfilt)` to `r nrow(res)`
features. We did remove a large amount of features but this is expected as we
want to focus on the most reliable features for our analysis.
For the rest of our analysis we need to separate the QC samples from the study
samples. We will store the QC samples in a separate object for later use.

```{r}
res_qc <- res[, res$phenotype == "QC"]
res <- res[, res$phenotype != "QC"]
```

Now our data set has been preprocessed, normalized and filtered we can now start
to understand the distribution of the data and estimate the variation due to
biology.

# Differential abundance analysis.

After normalization and quality control, the next step is to identify features
that are differentially abundant between the study groups. This crucial step
allows us to identify potential biomarkers or metabolites that are associated
with the study groups. In this section, we will perform a differential abundance
analysis  to identify significant features that are statistically more abundant
in either CVD or CTR

```{r}
#' Redefining colours to investigate variance origins
col_phenotype <- brewer.pal(4, name = "Dark2")[c(4, 3)]
names(col_phenotype) <- c("CVD",
                          "CTR")
col_sample <- col_phenotype[res$phenotype]
```

First let's observe how the different study groups separate after the
adjustments that were made to prevent variances other than biological.

```{r}
#' Log transform and scale the data for PCA analysis
vals <- assay(res, "norm_imputed") |>
    t() |>
    log2() |>
    scale(center = TRUE, scale = TRUE)
pca_res <- prcomp(vals, scale = FALSE, center = FALSE)

# Plot by phenotype - find way to plot side by side.
vals_st <- cbind(vals, phenotype = res$phenotype)
autoplot(pca_res, data = vals_st , colour = 'phenotype', scale = 0) +
    scale_color_manual(values = col_phenotype)
# Phili: PC3 and PC4 does not show very interesting info for the rest of the
# pipeline i think (?)
```

The PCA plot above shows that the study samples seem to be separated based on
phenotype. This is a good indication that there are differences in the
metabolite profiles between the two groups. However we can also see an outlier
in the control group. This variation has not been corrected by the previous
step and should be taken into account in the analysis.

```{r}
vals_st <- cbind(vals, age = res$age)
autoplot(pca_res, data = vals_st , colour = 'age', scale = 0)
#' i don't like the pca for age using base R, I can't make the legend a
#' gradient. but autoplot makes it complicated to have 2 plots next to
#' eachother, so not sure what to do for the publication. I'll try to figure
#' something out. Probably need to do a deep dive in ggplot. would be nice also
#' for later plots anyway.
#' jo: I guess we can drop that alltogether, there seems to be no age
#' dependency in the data.
#' phili: yess I kinda agree but age is the only biological difference between
#' the different samples. i think in term of workflow it's good to show what
#' people *should* do with their data.
```

The PCA above does not show any clear separation based on age. This is a good
indication that the age differences between the samples do not have a strong
impact on the metabolite profiles. We can therefore continue with the analysis.

To estimate the variation due to biological differences depicted in the first
PCA plot above, we will compute linear models for each metabolite. We will
utilize the `limma` package to conduct the differential abundance analysis.
This package is extensively employed for conducting differential expression
analysis in genomics and can be adapted for metabolomics data analysis as well.

This essentially involves multiple linear regression, but it should be applied
individually to each metabolite, as they are heavily correlated with each
other. In the model, we will only include age
(e.g. `model.matrix(~ phenotype + age)`) as it is the sole known variable
among our samples. We also set up a cut-off for significance at 0.05 and a
cut-off for log2 fold change at 0.5. The `lmFit` function with then fit a
linear model to each row (metabolite) of the data, explaining the metabolite
concentrations by phenotype and age. The `eBayes` function will then apply an
empirical Bayes smoothing to the standard errors of the estimated log-fold
changes.

```{r}
library(limma)
#' prep parameters
p.cut <- 0.05     # cut-off for significance.
m.cut <- 0.5      # cut-off for log2 fold change

age <- res$age
phenotype <- factor(res$phenotype)

#' Fit the linear model to the data, explaining metabolite
#' concentrations by phenotype and age.
design <- model.matrix(~ phenotype + age)
fit <- lmFit(log2(assay(res, "norm_imputed")), design = design)
fit <- eBayes(fit)
```

After the linear model has been generated, we can now proceed to extract the
results. We will create a data frame containing the coefficients, raw and
adjusted p-values (applying a Benjamini-Hochberg correction,
e.g. `method = "BH"` for improved control of the false discovery rate), the
average intensity of signals in CVD and CTR samples, and an indication of
whether a feature is deemed significant or not.

```{r}
tmp <- data.frame(
    coef.CVD = fit$coefficients[, "phenotypeCVD"],
    pvalue.CVD = fit$p.value[, "phenotypeCVD"],
    adjp.CVD = p.adjust(fit$p.value[, "phenotypeCVD"], method = "BH"),
    avg.CVD = rowMeans(
        log2(assay(res, "norm_imputed")[, res$phenotype == "CVD"])),
    avg.CTR = rowMeans(
        log2(assay(res, "norm_imputed")[, res$phenotype == "CTR"]))
)
tmp$significant.CVD <- abs(tmp$coef.CVD) > m.cut & tmp$adjp.CVD < p.cut
rowData(res) <- cbind(rowData(res), tmp)
```

By using `rowData(res) <-`, we can store the results of the differential
abundance analysis are now stored in the `SummarizedExperiment` object. We can
now proceed to visualize the distribution of the raw and adjusted p-values.

```{r histogram, echo = FALSE, fig.cap = "Distribution of raw (left) and adjusted p-values (right)."}
par(mfrow = c(1, 2))
hist(rowData(res)$pvalue.CVD, breaks = 64, xlab = "p value",
     main = "Distribution of raw p-values",
     cex.main = 1, cex.lab = 1, cex.axis = 1)
hist(rowData(res)$adjp.CVD, breaks = 64, xlab = expression(p[BH]~value),
     main = "Distribution of adjusted p-values",
     cex.main = 1, cex.lab = 1, cex.axis = 1)
```

The histograms above show the distribution of raw and adjusted p-values. The
adjusted p-values are more conservative and account for multiple testing. Which
is important here as we fit a linear model to each feature/metabolite and
therefore perform a large number of tests. We do see that some features have
very low p-values, indicating that they are likely to be significantly
different between the two study groups.

Below we will plot the adjusted p-values against the log2 fold change. This
volcano plot will allow us to visualize the features that are significantly
different between the two study groups. We will also display the number of
significant features and the most significant features.

```{r volcano, echo = FALSE, fig.cap = "Volcano plot showing the analysis results."}
#' Plot volcano plot of the statistical results
par(mfrow = c(1, 1), mar = c(5, 5, 5, 1))
plot(rowData(res)$coef.CVD, -log10(rowData(res)$adjp.CVD),
     xlab = expression(log[2]~difference),
     ylab = expression(-log[10]~p[BH]), pch = 16, col = "#00000060",
     cex.main = 1.5, cex.lab = 1.5, cex.axis = 1.3)
rect(xleft = -100, ybottom = -log10(p.cut), xright = -m.cut, ytop = 100,
     border = NA, col = paste0(brewer.pal(3, "Set1")[2], 10))
rect(xleft = m.cut, ybottom = -log10(p.cut), xright = 100, ytop = 100,
     border = NA, col = paste0(brewer.pal(3, "Set1")[2], 10))
if (any(rowData(res)$significant.CVD)) {
    points(rowData(res)$coef.CVD[rowData(res)$significant.CVD],
           -log10(rowData(res)$adjp.CVD[rowData(res)$significant.CVD]),
           col = "#0000ffcc")
}
```

```{r}
#' nb of significant features
sum(rowData(res)$significant.CVD)
```

We therefore have `r sum(rowData(res)$significant.CVD)` features that are
significantly different between the two study groups. Below we will display the
most significant features.

```{r result-table, echo = FALSE, results = "asis"}
# Table of significant features
tab <- rowData(res)[rowData(res)$significant.CVD,
                    c("mzmed", "rtmed", "coef.CVD", "adjp.CVD",
                      "avg.CTR", "avg.CVD")]
tab <- tab[order(abs(tab$coef.CVD), decreasing = TRUE), ]
tab <- cbind(tab,
             rsd_QC = rowRsd(assay(res_qc, "norm_imputed")[rownames(tab), ]))
pandoc.table(
    as.data.frame(tab), style = "rmarkdown", split.table = Inf,
    caption = "Features with significant differences in abundances.")
```

```{r}
# Phili: i woud actually remove that it's a bit messay and all the important info are in the table above.
data_sample <- data[sampleData(data)$phenotype != "QC", keepFeatures = TRUE]
fts_sign <- featureChromatograms(
    data_sample, features = rownames(tab), expandRt = 5, filled = TRUE)

for (i in rownames(fts_sign)) {
    tmp <- fts_sign[i,]
    pk_col <- col_phenotype[as.character(tmp$phenotype[chromPeaks(tmp)[, "column"]])]
    plotChromPeakDensity(tmp, peakPch = 16,
                         peakCol = paste0(pk_col, 80),
                         peakBg = paste0(pk_col, 10))
    legend("topright", col = col_phenotype,
           legend = names(col_phenotype), lty = 1, cex = 0.5)
}
```

We can see that the features that are significantly different between the two
study groups are all present in CTR samples and significantly lower/absent in
CVD samples. This is a good indication that these features could be potential
biomarkers for CVD. Another characteristic to note is the low intensity of some
of these features. This means that unfortunately these feature are probably not
going to be selected for MS2 fragmentation by the mass spectrometer, as the
set up will prioritize the most intense features. (that's right no even with the inclusion list ?)

```{r}
save(data, file = "data_after_DA.RData")
save(res, file = "Sum_Exp_afterDA.RData")
```

# Annotation

Now that we have establish a list of interesting feature for a potential
biomarker. We can now annotate these features to identify the compounds they
represent. Annotation can be performed at different level of confidence.
A low level confidence but fast matching is to annotate using the MS1 level
information of the significant features. If the user want higher confidence
annotation, they will use MS2 data. In this experiment MS2 spectra was captured
in a second run, using an inclusion list.
In this next section we will demonstrate multiple way to annotate our
significant features. As well as discussing alternative strategy based on the
data and the overall goal of the experiment.

## MS1 based annotation

As explained previously, we will initially demonstrate a simple and efficient
method to analyze significant features utilizing only MS1 level data. This
approach relies on the m/z value of the feature and will align it with a
database of known compounds. To achieve this, we will match the m/z values of
the resulting features to the MassBank database using the `MatchValues()`
function.

```{r}
load("data_after_DA.RData")
load("Sum_Exp_afterDA.RData")
```

We will first load the MassBank database using the packages *AnnotationHub* and
*CompoundDb* and extract the relevant information for the annotation process.

We will then define the parameters for the matching process using
`Mass2MzParam`. This function allows us to specify the `adducts` to consider,
the `tolerance` for the matching process, and the `ppm` error. The parameter
`mzColname()` here specficied is the column name in the `res` object that
contains the m/z values of the features. All of the function for matching that
are going ot be described below are part of the *MetaboAnnotation* package.
Finally, we will match the m/z values of the significant features to the
MassBank database and extract the relevant information for annotation.

```{r}
#' Packages specifically used for annotation
library(AnnotationHub)
library(CompoundDb)
library(MetaboAnnotation)

#' load reference data
ah <- AnnotationHub()
query(ah, "MassBank")
mb <- ah[["AH116166"]]

#' Extract info of interest
cmps <- compounds(mb, columns = c("compound_id", "name", "formula",
                                  "exactmass", "inchikey"))

#' Selecting significant features
rowData(res)$feature_id <- rownames(rowData(res))
res_sig <- res[rowData(res)$significant.CVD, ]

#' Perform matching
param <- Mass2MzParam(adducts = c("[M+H]+", "[M+Na]+", "[M+H-NH3]+"),
                      tolerance = 0, ppm = 5)

mtch <- matchValues(res_sig, cmps, param = param, mzColname = "mzmed")
mtch
```

Note above that we select for the latest update of the MassBank database when `
computing `mb <- ah[["AH116166"]]`.
The `Matched` object shows that 4 of our significant features that were matched
to the MassBank database.

Evaluating for each significant feature the hits to the database.

```{r}
#' keep only features that have matches
mtch <- mtch[unique(queryIndex(mtch))]

#' Extracting the results and only best
mtch_res <- matchedData(mtch, c("feature_id", "mzmed", "rtmed",
                                "adduct", "ppm_error",
                                "target_formula", "target_name", "target_inchikey"))
rownames(mtch_res) <- NULL

#' Keep only info on features that machted - create a utility function for that
mtch_res <- split(mtch_res, mtch_res$feature_id) |>
    lapply(function(x) {
        lapply(split(x, x$target_inchikey), function(z) {
            z[which.max(z$ppm_error), ]
        }) |>
            do.call(what = rbind)
    }) |>
    do.call(what = rbind)

#' Display the results
mtch_res |>
    as.data.frame() |>
    pandoc.table(style = "rmarkdown", caption = "MS1 annotation results",
                 split.table = Inf)
```

The table above shows the results of the annotation process. We can see that
four of our significant features were matched to the MassBank database. The
matches seem to be pretty accurate with low ppm errors. Although, reference
compounds have, for the same feature, the same chemical formula, their names as
well as their (reported) *exact mass* differs, leading to multiple hits per
feature.

Above we tried to reduce the number of these duplicated hit using the inchikey
information. The inchi or inchikey combine info from the chemical formula and
structure, so while different compounds can share the same chemical formula,
they should have a different structure and thus inch However, this is not
always possible and the user should be aware that the same compound can be
reported multiple times in the database.

## MS2 based annotation

MS1 annotation is a fast and efficient method to annotate features and therfore
give a first insight into the compounds that are significantly different between
the two study groups. However, it is not always the most accurate. MS2 data can
provide a higher level of confidence in the annotation process. This is because
MS2 data provides information on the fragmentation pattern of the compound,
which can be used to identify the compound with higher confidence.

In our analysis, we concluded that the significant features were exclusively
detected in the CTR samples and were absent in the CVD samples. Consequently,
we will utilize the MS2 data obtained from the CTR samples to annotate these
significant features. While most experiments use QC (Quality Control) samples
for this purpose, their usage can occasionally dilute the signal of interest.
Given that we have recorded MS2 data in both study samples and QC samples, we
can confidently rely on the CTR samples to ensure optimal signal strength,
thereby facilitating the best possible matches.

jo: mention that we eventually should/could align the data sets first.

```{r echo=TRUE}
#' Import run2 data
path <- "C:/Users/plouail/OneDrive - Scientific Network South Tyrol/end-to-end_worflow/data/mzML_files/MS2/"
filename <- list.files(path = path, pattern = "\\.mzML$")
res_ms2 <- readMsExperiment(file.path(path, filename))

#' same filtering as run1
res_ms2 <- filterRt(res_ms2, c(10, 240))

#' add quick phenodata
sampleData(res_ms2)$phenotype <- "CTR"
sampleData(res_ms2)$frag_method <- c("CE20", "CE30", "CES")

#' check the number of spectra per ms level
spectra(res_ms2) |>
    msLevel() |>
    split(fromFile(res_ms2)) |>
    lapply(table) |>
    do.call(what = cbind)
```

Here, we will set up the `Spectra` object for the second run containing
MS2 data. The identical procedure will be applied to the database spectra. This
preparation step efficiently filters the spectra data to retain only MS2 (using
`filterMsLevel()`) spectra while eliminating spectra with only a single peak.
Additionally, we will use `filterIntensity()` and `filterPrecursorPeaks()` to
filter the MS2 spectra to retain only peaks with greater intensity that are
below the precursor m/z value.

The `Spectra` also has its own metadata that can be retrieve using
`spectraData()` function. Here we will extract the fragmentation method used for
each MS2 spectra.

```{r prep spectra object}
# Remove low intensity peaks
low_int <- function(x, ...) {
    x > max(x, na.rm = TRUE) * 0.05
}

res_ms2 <- spectra(res_ms2)|>
    filterMsLevel(2) |>
    filterIntensity(intensity = low_int)

#' remove peaks >= precursor m/z
res_ms2 <- filterPrecursorPeaks(res_ms2, ppm = 50, mz = ">=")

# Remove spectra that have one peak only
res_ms2 <- res_ms2[lengths(res_ms2) > 1]

#' add fragmentation data to `spectraData`
res_ms2$frag_method <- regmatches(
    dataOrigin(res_ms2),
    regexpr("(CE20|CE30|CES)", dataOrigin(res_ms2)))
table(res_ms2$frag_method)

#' Extract spectra data from database
ref_ms2 <- Spectra(mb)

#' Do same filtering as for our spectra data
ref_ms2 <- filterIntensity(ref_ms2, intensity = low_int)
ref_ms2 <- filterPrecursorPeaks(ref_ms2, ppm = 50, mz = ">=")
ref_ms2 <- ref_ms2[lengths(ref_ms2) > 1]
```

Now that both the `Spectra` object for the second run and the database spectra
have been prepared, we can proceed with the matching process. The goal is to
identify the MS2 spectra from the second run that could represent fragments
of the ions of features from the data in the first run. Our approach is to
match MS2 spectra against the significant features determined earlier based on their precursor m/z and retention time (given an acceptable tolerance) to the
feature's *mzmed* and *rtmed*. This approach is quite fast and easy, moreover, by taking in account the `featureArea()` we effectively consider the actual *m/z*
and retention time ranges of the features' chromatographic peaks and therefore
increase the chance of finding a correct match.

In our case we want to annotate the significant features adn therefore will
reduce the `featureArea()` dataframe to the features wanted. In case your
experiment is run with DDA format and/or you want to annotate as many features
as possible, you can use the whole `featureArea()` dataframe.

To perform this matching step we will use the `filterRanges()` function. This
function will filter the MS2 spectra based on the retention time and precursor
m/z values of the significant features.

```{r}
#' Only extracting the spectra variables we actually need.
query <- spectraData(
    res_ms2, columns = c("rtime", "precursorMz", "frag_method"))

#' The target will be range of significant features using featureArea
idx <- rownames(res_sig)
target <- as.data.frame(featureArea(data)[idx,])

#' Use this in new filterRanges function (use apply)
filt_fts <- apply(target[, c("rtmin", "rtmax", "mzmin", "mzmax")], MARGIN = 1,
              FUN = filterRanges, object = res_ms2,
              spectraVariables = c("rtime", "precursorMz"))
#' Phili: Is it possible to not have a list as an output ?
#' Does it even make sense to not have a list ?
#' jo: combine them again with c to have a single Spectra.

# remove null matches
nna <- lapply(filt_fts, function(x) length(x) > 1)
nna <- which(unlist(nna))

# keep only non null matches
filt_fts <- filt_fts[nna]

# found ms2 matches for
length(filt_fts)
```

We now have a list with spectra for each feature. We can now match these spectra
against the MassBank database. We will use the `CompareSpectraParam` function to
define the parameters for the matching process. We will set the `ppm` to 20, the
`tolerance` to 0.1, and require the precursor to be present. We will also set a
threshold function to only keep matches with a score greater than 0.7.

```{r}
prm <- CompareSpectraParam(ppm = 20, tolerance = 0.1, requirePrecursor = TRUE,
                           THRESHFUN = function(x) which(x >= 0.6))

register(SerialParam())

#' match
mtchList_fct <- function(x) {
    mtch <- matchSpectra(x, ref_ms2, param = prm)
    if (length(whichQuery(mtch)) > 0) {
        mtch <- mtch[whichQuery(mtch)]
        mtch
    } else mtch <- NULL
}

mtch_list <- lapply(filt_fts, mtchList_fct)

#remove null matches
mtch_list <- mtch_list[!vapply(mtch_list, is.null, logical(1))]

#' number of feature matched
length(mtch_list)
```

We have successfully matched `r length(mtch_list)` features to the MassBank.
maching to databases are usual bottleneck in the analysis workflow. We advise
match to multiple databases,as well as using other software (e.g. *SIRUS*).
Below we will extract the results and display the most significant matches.

```{r}
#' Extracting the results + remove duplicate ?
md <- matchedData(mtch_list[[1]], columns = c("target_inchikey", "score",
                                              "rtime", "frag_method",
                                              "target_name"))

#' As before we remove duplicates
md <- lapply(split(md, md$target_inchikey), function(z) {
    z[which.max(z$score), ]
}) |>
    do.call(what = rbind) |>
    as.data.frame()

pandoc.table(md, style = "rmarkdown", caption = "MS2 annotation results",
             split.table = Inf)
```

The provided table illustrates the outcomes of the annotation process. It
reveals that the MS2 spectra of the noteworthy features were compared to the
MassBank database, resulting in only one significant match identified,
corresponding to the Caffeine compound. To gain further insights, we aim to
plot the locations where the MS2 spectra were recorded, juxtaposed with the
feature of interest.

```{r}
#plot EICs and MS2 spectra
idx <- sampleData(data)$phenotype != "QC"
eics_is <- featureChromatograms(data[idx, keepFeatures = TRUE], features = names(mtch_list), expandRt = 1)
plot(eics_is, col = col_sample, main = paste0(names(mtch_list), ": Caffeine"),
     peakBg = paste0(col_sample[chromPeaks(eics_is)[, "sample"]], 40),
     lwd = 2)
abline(v = md$rtime)
grid()
legend("topright", col = col_phenotype,
       legend = names(col_phenotype), lty = 1, bty = "n")


##Phili: not sure what you think of this whole annotation thing way, the list
# of spectra/ list of
#'matches may be a bit confusing ? Do you have a better way of doing that ? Or
#'should i keep to the  "normal" way and not use fitlerRanges() in the first
#'place ?

#' are you fine with having one match at the end ? It's a nice story i guess
```


# Summary

In summary, we have successfully preprocessed the data, normalized it,
and filtered out unreliable features. Through our analysis, we have
identified features that exhibit significant differences between the two
study groups. Utilizing both MS1 and MS2 data, we have annotated these
significant features, enabling us to visualize the results and offer
insights into potential biomarkers for Cardiovascular Disease (CVD).
Notably, our findings indicate a significant presence of caffeine in the
control group and its absence in the CVD group.

However, upon further
consideration, we must recognize that individuals with cardiovascular
disease are generally advised to refrain from ingesting caffeine.
Consequently, higher levels of caffeine observed in the control group may
not necessarily denote it as a biomarker for CVD, but rather as a
consequence of the disease. This underscores the critical importance of
experimental design in analysis, highlighting that inferring conclusions
from results is not always as straightforward as initially perceived.

# Session information

```{r}
sessionInfo()
```

# References

# Appendix

## Alignment using manually selected anchor peaks

- align the data set using internal standards.
- suggested to eventually *enrich* the anchor peaks with signal from other ions
  in retention time regions not covered by the internal standards.

# Additional informations

```{r eval=FALSE, include=FALSE}
plot(eics_is_noprocess)

#' Notes on the EICs:
#' - Alanine 13C 15N: signal very low, maybe other ion?
#' - Arginine: very nice signal, RT a bit off.
#' - Aspartic acid: very nice signal.
#' - Carnitine: nothing there, maybe other ion?
#' - Creatinine: nothing there, maybe other ion?
#' - Cystine: very nice signal.
#' - Glucose: nice signal.
#' - Glutamic acid: very nice signal.
#' - Glycine: signal low but ~ OK.
#' - cystine: very nice signal, RT a bit off.
#' - Isoleucine: multiple peaks, most likely both leucine and isoleucine.
#' - Leucine: same as Isoleucine.
#' - Methionine: nice signal, but some other peaks close by.
#' - Phenylalanine: nice signal, but some other peaks close by.
#' - Proline: signal low but ~ OK (maybe other ion?)
#' - Serine: nice signal.
#' - Threonine: nice signal.
#' - Tyrosine: signal low but ~ OK.
#' - Valine: signal OK. Other peaks close by.
#'

plot(eics_is_chrompeaks) # show chrompeak detection
plot(eics_is_refined) # show refinement effect
plot(eic_is) # show alignment effect
```

```{r}
#possible extra info:
# -
```
